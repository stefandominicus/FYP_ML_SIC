{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Acquisition & Processing",
      "provenance": [],
      "collapsed_sections": [
        "PRK6-M9FTktG",
        "LRTDOjJfcvNJ",
        "NTI24fiVTwqr",
        "ErULNVtViVZI",
        "vVaBydhfo37f",
        "SfcA9sgUq5X0",
        "0bkZiOEdoKpt",
        "gMzckDO-4prs",
        "J_eoZospiWwc",
        "w8bejtKmJ3rm",
        "yqHHQs3txiSZ",
        "0IBNvazrxpsV",
        "An-u-PqxxEq1",
        "WzIKnpYB_Qyc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S7P82Aq8vkU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Run to Setup\n",
        "import os\n",
        "import cv2\n",
        "import requests\n",
        "import ftplib\n",
        "\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopy.distance as geoDist\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "try:\n",
        "  import sentinelsat\n",
        "except:\n",
        "  !pip install sentinelsat\n",
        "  import sentinelsat\n",
        "  \n",
        "try:\n",
        "  import netCDF4 as nc\n",
        "except:\n",
        "  !pip install netCDF4\n",
        "  import netCDF4 as nc\n",
        "\n",
        "# Mount drive\n",
        "clear_output()\n",
        "print(\"Mount Google Drive\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Global Variables\n",
        "image_dimension = 128\n",
        "\n",
        "# OData Authentication\n",
        "# Enter your credentials here\n",
        "# odata_username = \"\"\n",
        "# odata_password = \"\"\n",
        "search_api = sentinelsat.SentinelAPI(odata_username, odata_password)\n",
        "\n",
        "# CMEMS Authentication\n",
        "# Enter your credentials here\n",
        "# cmems_username = \"\"\n",
        "# cmems_password = \"\"\n",
        "\n",
        "# Dataset folder names\n",
        "dataset_location_path   = \"/content/gdrive/Shared drives/ICE_CHARTING_UCT_WSA/Data\"\n",
        "\n",
        "#SENTINEL-1 Quicklook File Save Name\n",
        "sentinel_1_folder         = \"SENTINEL-1 SAR\"\n",
        "sentinel_1_quicklook_name = \"Quick Look Images/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "sentinel_1_resampled_name = \"Resampled Images/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "\n",
        "# CMEMS File Save Name\n",
        "cmems_label_folder = \"CMEMS Concentration\"\n",
        "cmems_name         = \"NC Files/{0}/{1}/{0}_{1}_{2}.nc\" # 0:YEAR, 1:MONTH, 2:DAY\n",
        "concentration_name = \"Concentration Labels/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "uncertainty_name   = \"Uncertainty Labels/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "\n",
        "# Validation\n",
        "validations_folder    = \"Image Processing Validation\"\n",
        "interpolation_name    = \"Interpolation Validation/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "label_comparison_name = \"Patch Label Validation/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "\n",
        "# Show the available datasets\n",
        "clear_output()\n",
        "dataset_location_path   = \"/content/gdrive/Shared drives/ICE_CHARTING_UCT_WSA/Data\"\n",
        "print(\"The following datasets are available:\")\n",
        "for name in os.listdir(dataset_location_path):\n",
        "  print(\"\\t\" + name)\n",
        "print(\"Please make sure that the new dataset name is unique.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBd3G8NI01Iv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Step 1: Dataset Parameters\n",
        "#@markdown ---\n",
        "#@markdown ###Choose a name for this dataset\n",
        "dataset_name = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ###Select the start date (included)\n",
        "start_date = \"2019-01-01\" #@param {type:\"date\"}\n",
        "#@markdown ###Select the end date (excluded)\n",
        "end_date   = \"2019-07-01\" #@param {type:\"date\"}\n",
        "#@markdown ---\n",
        "#@markdown ###Choose region of interest\n",
        "ROI = \"Arctic Circle\" #@param [\"Arctic Circle\", \"Antarctic Circle\", \"South Sandwich Islands\", \"Other North\", \"Other South\"]\n",
        "#@markdown ---\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "# Dataset Information\n",
        "start_date   = start_date.replace(\"-\", \"\")\n",
        "end_date     = end_date.replace(\"-\", \"\")\n",
        "dataset_name = \"{}_{}_{}\".format(dataset_name, start_date, end_date)\n",
        "\n",
        "os.chdir(dataset_location_path)\n",
        "if dataset_name not in os.listdir():\n",
        "  # Make the dataset folder\n",
        "  os.mkdir(dataset_name)\n",
        "\n",
        "  # Make the folders for label comparisons\n",
        "  # os.mkdir(os.path.join(dataset_name, validations_folder))\n",
        "  # os.mkdir(os.path.join(dataset_name, validations_folder, \"Interpolation Validation\"))\n",
        "  # os.mkdir(os.path.join(dataset_name, validations_folder, \"Patch Label Validation\"))\n",
        "  \n",
        "  # Done making new folders.\n",
        "  print(\"Made new directory.\")\n",
        "else:\n",
        "  print(\"Directory already exists.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRK6-M9FTktG",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: SENTINEL-1 Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgwALtOhU_cE",
        "colab_type": "text"
      },
      "source": [
        "**Search**\n",
        "\n",
        "Query the Copernicus database using the Sentinelsat API (which uses OpenSearch)\n",
        "\n",
        "This produces a dictionary of all search results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdTgt_x_0y37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create directory for SENTINEL data\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if sentinel_1_folder not in os.listdir(): os.mkdir(sentinel_1_folder)\n",
        "\n",
        "# Define ROI\n",
        "# Useful site for generating and checking these polygons: https://arthur-e.github.io/Wicket/sandbox-gmaps3.html\n",
        "roi_polygons = {\"Arctic Circle\"           : \"POLYGON((-180 66,-180 85,0 85,180 85,180 66,0 66,-180 66))\",\n",
        "                \"Antarctic Circle\"        : \"POLYGON((-180 -66,-180 -85,0 -85,180 -85,180 -66,0 -66,-180 -66))\",\n",
        "                \"South Sandwich Islands\"  : \"POLYGON((-29.30712890625 -55.480494204910514,-25.966796875 -55.975418279377394,-25.36279296875 -59.589097877324384,-28 -60,-29.30712890625 -55.480494204910514))\",\n",
        "                \"Other North\"             : \"<Enter a Northern hemisphere polygon here if you want to define some other region>\",\n",
        "                \"Other South\"             : \"<Enter a Southern hemisphere polygon here if you want to define some other region>\"}\n",
        "\n",
        "# Define additional keyword search items\n",
        "kwargs = {'platformname':'Sentinel-1',\n",
        "          'producttype':'GRD',\n",
        "          'sensoroperationalmode':'EW',\n",
        "          'polarisationmode':'HH+HV'\n",
        "         }\n",
        "\n",
        "# Query for matching products\n",
        "while (True):\n",
        "  try:\n",
        "    # This often times out, so try is as many times as is necessary, unless the user cancels\n",
        "    products = search_api.query(area=roi_polygons[ROI],\n",
        "                                date=(start_date, end_date),\n",
        "                                # date=(\"20190101\", \"20190301\"),\n",
        "                                limit=None,\n",
        "                                offset=0,\n",
        "                                **kwargs)\n",
        "    break\n",
        "  except KeyboardInterrupt:\n",
        "    break\n",
        "  except:\n",
        "    print(\"Timed out - trying again.\")\n",
        "\n",
        "print(\"\\nThere are {} results matching the search criteria.\".format(len(products)))\n",
        "\n",
        "# Write all search results uuids to txt file\n",
        "os.chdir(sentinel_1_folder)\n",
        "with open(\"Search Results.txt\", 'w') as f:\n",
        "  for uuid in products.keys():\n",
        "    f.write(uuid + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmgEaombVve7",
        "colab_type": "text"
      },
      "source": [
        "**Download**\n",
        "\n",
        "Save all quicklook files to Google Drive (retry failed downloads)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj1yWuH1V8-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root access path\n",
        "# service_root_URI = \"https://scihub.copernicus.eu/dhus/odata/v1/\"\n",
        "service_root_URI = \"https://scihub.copernicus.eu/apihub/odata/v1/\"\n",
        "\n",
        "# Move into the SENTINEL-1 folder\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "\n",
        "# Remove the failed downlaods file so that those items can be retried\n",
        "if \"Failed_Downloads.txt\" in os.listdir(): os.remove(\"Failed_Downloads.txt\")\n",
        "\n",
        "# Lists to track downloads\n",
        "successful_downloads = []\n",
        "needs_downloading = {}\n",
        "\n",
        "# Do not want to re-download files already on the drive, so remove them from the list\n",
        "if \"Successful_Downloads.txt\" in os.listdir():\n",
        "  with open(\"Successful_Downloads.txt\", 'r') as f:\n",
        "    successful_downloads = [item.split(\"_\")[-1] for item in f.read().splitlines()] # Extract the UUID from the file name\n",
        "\n",
        "# All failed downloads will be retried automatically if they are contained in 'products'\n",
        "for uuid in products:\n",
        "  if uuid not in successful_downloads:\n",
        "    needs_downloading[uuid] = products[uuid]\n",
        "\n",
        "# Download all items which haven't already been downloaded\n",
        "for uuid, value in needs_downloading.items():\n",
        "  # clear_output()\n",
        "  print(\"Downloading product {} of {}: {}\\n\".format(list(needs_downloading.keys()).index(uuid) + 1, len(needs_downloading), uuid))\n",
        "  # quicklook_url = os.path.join(service_root_URI, \"Products('{}')/Products('Quicklook')/$value\".format(uuid))\n",
        "  quicklook_url = value['link_icon']\n",
        "  print(quicklook_url)\n",
        "  response = requests.get(quicklook_url, auth=(odata_username, odata_password))\n",
        "  if (response.status_code == 200):\n",
        "    date = value['endposition']\n",
        "    if not os.path.exists(\"Quick Look Images/{}/{}/{}\".format(date.year, date.month, date.day)): os.makedirs(\"Quick Look Images/{}/{}/{}\".format(date.year, date.month, date.day))\n",
        "    if not os.path.exists(\"Resampled Images/{}/{}/{}\".format(date.year, date.month, date.day)): os.makedirs(\"Resampled Images/{}/{}/{}\".format(date.year, date.month, date.day))\n",
        "    try:\n",
        "      # Save the quicklook image\n",
        "      open(sentinel_1_quicklook_name.format(date.year, date.month, date.day, uuid), 'wb').write(response.content)\n",
        "      original_image = cv2.imread(sentinel_1_quicklook_name.format(date.year, date.month, date.day, uuid))\n",
        "\n",
        "      # Save a resampled version of the image\n",
        "      cv2.imwrite(sentinel_1_resampled_name.format(date.year, date.month, date.day, uuid), cv2.resize(original_image, (image_dimension, image_dimension)))\n",
        "\n",
        "      # Add this uuid to the list of successful downloads\n",
        "      with open(\"Successful_Downloads.txt\", 'a+') as f:\n",
        "        f.write(\"{}_{}_{}_{}\\n\".format(date.year, date.month, date.day, uuid))\n",
        "    except:\n",
        "      # Failed when trying to write save image files\n",
        "      with open(\"Failed_Downloads.txt\", 'a+') as f:\n",
        "        f.write(\"{}_{}_{}_{}\\n\".format(date.year, date.month, date.day, uuid))\n",
        "  else:\n",
        "    # Bad response from server\n",
        "    print(\"Bad response from server\")\n",
        "    with open(\"Failed_Downloads.txt\", 'a+') as f:\n",
        "      f.write(\"{}_{}_{}_{}\\n\".format(date.year, date.month, date.day, uuid))\n",
        "\n",
        "# Sumarise\n",
        "print(\"Done saving search data.\")\n",
        "if \"Failed_Downloads.txt\" in os.listdir():\n",
        "  with open(\"Failed_Downloads.txt\", 'r') as f:\n",
        "    failed_downloads = f.read().splitlines()\n",
        "else:\n",
        "  failed_downloads = []\n",
        "print(\"Tried to save {} images, {} failed.\".format(len(needs_downloading), len(failed_downloads)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRTDOjJfcvNJ",
        "colab_type": "text"
      },
      "source": [
        "## Delete all SENTINEL-1 data - Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWK5URrWIKsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if sentinel_1_folder in os.listdir():\n",
        "  os.chdir(sentinel_1_folder)\n",
        "\n",
        "  if \"Search Results.txt\" in os.listdir(): os.remove(\"Search Results.txt\")\n",
        "  if \"Successful_Downloads.txt\" in os.listdir(): os.remove(\"Successful_Downloads.txt\")\n",
        "  if \"Failed_Downloads.txt\" in os.listdir(): os.remove(\"Failed_Downloads.txt\")\n",
        "\n",
        "  if \"Quick Look Images\" in os.listdir():\n",
        "    for y in os.listdir(\"Quick Look Images\"):\n",
        "      for m in os.listdir(os.path.join(\"Quick Look Images\", y)):\n",
        "        for d in os.listdir(os.path.join(\"Quick Look Images\", y, m)):\n",
        "          for f in os.listdir(os.path.join(\"Quick Look Images\", y, m, d)):\n",
        "            os.remove(os.path.join(\"Quick Look Images\", y, m, d, f))\n",
        "          os.rmdir(os.path.join(\"Quick Look Images\", y, m, d))\n",
        "        os.rmdir(os.path.join(\"Quick Look Images\", y, m))\n",
        "      os.rmdir(os.path.join(\"Quick Look Images\", y))\n",
        "    os.rmdir(\"Quick Look Images\")\n",
        "\n",
        "  if \"Resampled Images\" in os.listdir():\n",
        "    for y in os.listdir(\"Resampled Images\"):\n",
        "      for m in os.listdir(os.path.join(\"Resampled Images\", y)):\n",
        "        for d in os.listdir(os.path.join(\"Resampled Images\", y, m)):\n",
        "          for f in os.listdir(os.path.join(\"Resampled Images\", y, m, d)):\n",
        "            os.remove(os.path.join(\"Resampled Images\", y, m, d, f))\n",
        "          os.rmdir(os.path.join(\"Resampled Images\", y, m, d))\n",
        "        os.rmdir(os.path.join(\"Resampled Images\", y, m))\n",
        "      os.rmdir(os.path.join(\"Resampled Images\", y))\n",
        "    os.rmdir(\"Resampled Images\")\n",
        "\n",
        "  if len(os.listdir()) == 0:\n",
        "    os.chdir(\"..\")\n",
        "    os.rmdir(sentinel_1_folder)\n",
        "\n",
        "  print(\"All files should have been deleted.\")\n",
        "else:\n",
        "  print(\"No SENTINEL-1 data to delete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTI24fiVTwqr",
        "colab_type": "text"
      },
      "source": [
        "# Step 3: CMEMS Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4BgR_4iZxpN",
        "colab_type": "text"
      },
      "source": [
        "**Download**\n",
        "\n",
        "Connect to the FTP server, and save the required .nc files to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFQbLskNZwm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create directory for CMEMS data\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if cmems_label_folder not in os.listdir(): os.mkdir(cmems_label_folder)\n",
        "os.chdir(cmems_label_folder)\n",
        "\n",
        "# These ROIs will be treated as norhtern locations, all others will be assumed to be in the South\n",
        "north_keys = [\"Arctic Circle\", \"Other North\"]\n",
        "\n",
        "# Product information (Adjust directory names based on which hemisphere has been selected in the ROI)\n",
        "server_name    = \"nrt.cmems-du.eu\"\n",
        "product_family = \"SEAICE_GLO_SEAICE_L4_NRT_OBSERVATIONS_011_001\"\n",
        "product_name   = \"METNO-GLO-SEAICE_CONC-{}-L4-NRT-OBS\".format(\"NORTH\" if ROI in north_keys else \"SOUTH\")\n",
        "file_on_server = \"{0}/{1}/ice_conc_{3}h_polstere-100_multi_{0}{1}{2}1200.nc\".format(\"{0:04d}\", \"{1:02d}\", \"{2:02d}\", \"n\" if ROI in north_keys else \"s\")\n",
        "\n",
        "# Start date\n",
        "start_datetime = dt.datetime(int(start_date[:4]), int(start_date[4:6]), int(start_date[6:]))\n",
        "\n",
        "# End date (inclusive)\n",
        "end_datetime   = dt.datetime(int(end_date[:4]), int(end_date[4:6]), int(end_date[6:]))\n",
        "\n",
        "# Lists to track downloads\n",
        "successful_downloads = []\n",
        "failed_downloads = []\n",
        "\n",
        "# Don't want to repeat already downloaded products\n",
        "if \"Successful_Downloads.txt\" in os.listdir():\n",
        "  with open(\"Successful_Downloads.txt\", 'r') as file:\n",
        "    successful_downloads = file.read().splitlines()\n",
        "\n",
        "# Connect to the CMEMS FTP server\n",
        "server = ftplib.FTP()\n",
        "server.connect(server_name)\n",
        "server.login(cmems_username, cmems_password)\n",
        "\n",
        "# Locate the desired product\n",
        "server.cwd(\"Core\")\n",
        "server.cwd(product_family)\n",
        "server.cwd(product_name)\n",
        "\n",
        "# Iterate through the desired files\n",
        "date = start_datetime\n",
        "download_count = 0\n",
        "while date < end_datetime:\n",
        "  if \"{}_{}_{}\".format(date.year, date.month, date.day) not in successful_downloads:\n",
        "    # File has not already been downloaded, so download it now\n",
        "    # clear_output()\n",
        "    print(\"Downloading file number {0}: {1:04d}-{2:02d}-{3:02d}\".format(download_count + 1, date.year, date.month, date.day))\n",
        "    if not os.path.exists(\"NC Files/{}/{}\".format(date.year, date.month)): os.makedirs(\"NC Files/{}/{}\".format(date.year, date.month))\n",
        "    try:\n",
        "      server.retrbinary(\"RETR \" + file_on_server.format(date.year, date.month, date.day), open(cmems_name.format(date.year, date.month, date.day), 'wb').write)\n",
        "      print (\"File saved: \" + cmems_name.format(date.year, date.month, date.day))\n",
        "      successful_downloads.append(\"{}_{}_{}\".format(date.year, date.month, date.day))\n",
        "      download_count += 1\n",
        "    except:\n",
        "      print(\"Download failed: \" + cmems_name.format(date.year, date.month, date.day))\n",
        "      failed_downloads.append(\"{}_{}_{}\".format(date.year, date.month, date.day))\n",
        "  date += dt.timedelta(days=1)\n",
        "\n",
        "# Disconnect from the FTP server\n",
        "server.quit()\n",
        "\n",
        "# Save download logs to csv\n",
        "with open(\"Successful_Downloads.txt\", 'w') as f:\n",
        "  for item in successful_downloads:\n",
        "    f.write(item + \"\\n\")\n",
        "if len(failed_downloads) > 0:\n",
        "  with open(\"Failed_Downloads.txt\", 'w') as f:\n",
        "    for item in failed_downloads:\n",
        "      f.write(item + \"\\n\")\n",
        "    \n",
        "# Update the user\n",
        "if download_count > 0:\n",
        "  print(\"Done Saving Files! {} new files downloaded.\".format(download_count))\n",
        "else:\n",
        "  print(\"No new files were downloaded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErULNVtViVZI",
        "colab_type": "text"
      },
      "source": [
        "## Delete all CMEMS data - Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUu1hLsXiami",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if cmems_label_folder in os.listdir():\n",
        "  os.chdir(cmems_label_folder)\n",
        "\n",
        "  if \"Successful_Downloads.txt\" in os.listdir(): os.remove(\"Successful_Downloads.txt\")\n",
        "  if \"Failed_Downloads.txt\" in os.listdir(): os.remove(\"Failed_Downloads.txt\")\n",
        "\n",
        "  if \"NC Files\" in os.listdir():\n",
        "    for y in os.listdir(\"NC Files\"):\n",
        "      for m in os.listdir(os.path.join(\"NC Files\", y)):\n",
        "        for f in os.listdir(os.path.join(\"NC Files\", y, m)):\n",
        "          os.remove(os.path.join(\"NC Files\", y, m, f))\n",
        "        os.rmdir(os.path.join(\"NC Files\", y, m))\n",
        "      os.rmdir(os.path.join(\"NC Files\", y))\n",
        "    os.rmdir(\"NC Files\")\n",
        "\n",
        "  if len(os.listdir()) == 0:\n",
        "    os.chdir(\"..\")\n",
        "    os.rmdir(cmems_label_folder)\n",
        "\n",
        "  print(\"All files should have been deleted.\")\n",
        "else:\n",
        "  print(\"No CMEMS data to delete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVaBydhfo37f",
        "colab_type": "text"
      },
      "source": [
        "# Step 4: Patch-wise Labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ACY67yZKYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_label(year, month, day, image, odata, nc_file):\n",
        "  # Get the footprint information, so that we can interpolate the coordinates of each pixel\n",
        "  # NOTE: This uses the 'footprint' provided by OData, in which the order of the points has been consistently\n",
        "  # manipulated along with the image, so the order of the points is consistent in the satellite frame of reference.\n",
        "  # This is different to the OpenSearch 'footprint', where the points are ordered in the global frame, and thus\n",
        "  # do not follow any flips/rotations performed to the image as part of the ESA preprocessing.\n",
        "  footprint = odata['footprint'][9:-2].split(\",\")\n",
        "  footprint = [(float(footprint[i].split(\" \")[1]), float(footprint[i].split(\" \")[0])) for i in range(4)]\n",
        "\n",
        "  # The OData 'footprint' is always ordered in according to the satellite frame\n",
        "  # of reference. When the images are flipped/rotated in preprocessing, the\n",
        "  # 'footprint' points remain ordered in the satellite frame orietation. The\n",
        "  # 'footprint' consists of 5 points: [(0), (1), (2), (3), (0)], and they\n",
        "  # are ordered as follows (in the satellite frame):\n",
        "  #\n",
        "  #                 (0)                 (1)\n",
        "  #                     +-------------+\n",
        "  #                     |             |\n",
        "  #                     |             |\n",
        "  #                     |             |\n",
        "  #                     |             |\n",
        "  #                     +-------------+\n",
        "  #                 (3)                 (2)\n",
        "  #\n",
        "  # So, if the latitude of (1) is greater than the latitude of (2), then the\n",
        "  # image was acquired during an ASCENDING pass, otherwise it was during a\n",
        "  # DESCENDING pass. This can be used to determin the pass direction without\n",
        "  # relying on the 'orbit direction' entry which seems to be unreliable in both\n",
        "  # OData and OpenSearch APIs.\n",
        "  if footprint[1][0] > footprint[2][0] and footprint[2][1] < footprint[0][1]:\n",
        "    # The satellite was definitely ASCENDING, and the footprint straddles the date line\n",
        "    for i in range(4): footprint[i] = (footprint[i][0], footprint[i][1] + (footprint[i][1] // -360) * 360)\n",
        "  elif footprint[2][0] > footprint[1][0] and footprint[3][1] < footprint[1][1]:\n",
        "    # The satellite was definitely DESCENDING, and the footprint straddles the date line\n",
        "    for i in range(4): footprint[i] = (footprint[i][0], footprint[i][1] + (footprint[i][1] // -360) * 360)\n",
        "  \n",
        "  top_left_lat     = footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2][0]\n",
        "  top_left_lon     = footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2][1]\n",
        "  top_right_lat    = footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3][0]\n",
        "  top_right_lon    = footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3][1]\n",
        "  bottom_right_lat = footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0][0]\n",
        "  bottom_right_lon = footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0][1]\n",
        "  bottom_left_lat  = footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1][0]\n",
        "  bottom_left_lon  = footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1][1]\n",
        "  # OData 'pass direction' works reliably in the section above, because any mistakes happen twice\n",
        "  # and effectively cancel out (although the image and label will be 180 degrees rotated from the\n",
        "  # global frame). To deal with the issue of crossing the date line, we need to know for sure which\n",
        "  # direction the pass was in (ASCENDING/DESCENDING). This will figure that out, and correct for\n",
        "  # negative lon values causing problems when interpolating.\n",
        "  \n",
        "  # Create linspace objects for left and right edges so that the rows can be iterated later\n",
        "  left_edge  = [np.linspace(top_left_lat,  bottom_left_lat,  image_dimension), np.linspace(top_left_lon,  bottom_left_lon,  image_dimension)]\n",
        "  right_edge = [np.linspace(top_right_lat, bottom_right_lat, image_dimension), np.linspace(top_right_lon, bottom_right_lon, image_dimension)]\n",
        "\n",
        "  # Extract the relevent tables from the .nc file\n",
        "  full_concentration = nc_file.variables['ice_conc'][0, :, :]\n",
        "  full_uncertainty   = nc_file.variables['total_uncertainty'][0, :, :]\n",
        "  full_lat = np.array(nc_file.variables['lat'])\n",
        "  full_lon = np.array(nc_file.variables['lon'])\n",
        "\n",
        "  # Compute the bounds of the image on the concentration map to reduce computational requirements later on\n",
        "  index_values = np.zeros((4, 2), dtype=np.uint16)\n",
        "  # top_left\n",
        "  distance_from_pixel = (full_lat - top_left_lat)**2 + (full_lon - (top_left_lon - 360 * ((top_left_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n",
        "  index = distance_from_pixel.argmin()\n",
        "  index_values[0, 0], index_values[0, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n",
        "  # top_right\n",
        "  distance_from_pixel = (full_lat - top_right_lat)**2 + (full_lon - (top_right_lon - 360 * ((top_right_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n",
        "  index = distance_from_pixel.argmin()\n",
        "  index_values[1, 0], index_values[1, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n",
        "  # bottom_left\n",
        "  distance_from_pixel = (full_lat - bottom_left_lat)**2 + (full_lon - (bottom_left_lon - 360 * ((bottom_left_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n",
        "  index = distance_from_pixel.argmin()\n",
        "  index_values[2, 0], index_values[2, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n",
        "  # bottom_right\n",
        "  distance_from_pixel = (full_lat - bottom_right_lat)**2 + (full_lon - (bottom_right_lon - 360 * ((bottom_right_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n",
        "  index = distance_from_pixel.argmin()\n",
        "  index_values[3, 0], index_values[3, 1] = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n",
        "  # Find the reduced size arrays\n",
        "  min_y = min(index_values[:, 0])\n",
        "  min_x = min(index_values[:, 1])\n",
        "  max_y = max(index_values[:, 0])\n",
        "  max_x = max(index_values[:, 1])\n",
        "  # Slice the full arrays\n",
        "  reduced_concentration = full_concentration[min_y:max_y, min_x:max_x]\n",
        "  reduced_uncertainty   = full_uncertainty[min_y:max_y, min_x:max_x]\n",
        "  reduced_lat = full_lat[min_y:max_y, min_x:max_x]\n",
        "  reduced_lon = full_lon[min_y:max_y, min_x:max_x]\n",
        "\n",
        "  # Create the label arrays\n",
        "  concentration_label = np.zeros((image_dimension, image_dimension), dtype=np.float_)\n",
        "  uncertainty_label   = np.zeros((image_dimension, image_dimension), dtype=np.float_)\n",
        "\n",
        "  # Iterate through each pixel and find the corresponding value for the label\n",
        "  for y_pixel in range(image_dimension):\n",
        "    interpolated_row = [np.linspace(left_edge[0][y_pixel], right_edge[0][y_pixel], image_dimension), np.linspace(left_edge[1][y_pixel], right_edge[1][y_pixel], image_dimension)]\n",
        "    for x_pixel in range(image_dimension):\n",
        "      if image[y_pixel, x_pixel] != 0:\n",
        "        # Only compute the concentration for non-zero pixels in the image (i.e. ignore black edges)\n",
        "        pixel_lat = interpolated_row[0][x_pixel]\n",
        "        pixel_lon = interpolated_row[1][x_pixel]\n",
        "        # Find the closest coordinate to this pixel, and use its value\n",
        "        distance_from_pixel = (reduced_lat - pixel_lat)**2 + (reduced_lon - (pixel_lon - 360 * ((pixel_lon + 180) // 360)))**2 # This is not Euclidian distance, but should be faster and still preserve order\n",
        "        index = distance_from_pixel.argmin()\n",
        "        y, x = index // distance_from_pixel.shape[1], index % distance_from_pixel.shape[1]\n",
        "        concentration_label[y_pixel, x_pixel] = 100 if reduced_concentration.mask[y, x] else reduced_concentration[y, x]\n",
        "        uncertainty_label[y_pixel, x_pixel]   = 0 if reduced_uncertainty.mask[y, x] else reduced_uncertainty[y, x]\n",
        "\n",
        "  # Save the label patches\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "  if not os.path.exists(\"Concentration Labels/{}/{}/{}\".format(year, month, day)): os.makedirs(\"Concentration Labels/{}/{}/{}\".format(year, month, day))\n",
        "  if not os.path.exists(\"Uncertainty Labels/{}/{}/{}\".format(year, month, day)): os.makedirs(\"Uncertainty Labels/{}/{}/{}\".format(year, month, day))\n",
        "  cv2.imwrite(concentration_name.format(year, month, day, odata['id']), concentration_label)\n",
        "  cv2.imwrite(uncertainty_name.format(year, month, day, odata['id']), uncertainty_label)\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "# Read in the list of successfully downloaded images\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "if \"Successful_Downloads.txt\" in os.listdir():\n",
        "  with open(\"Successful_Downloads.txt\", 'r') as file:\n",
        "    needs_labelling = file.read().splitlines()\n",
        "  print(\"Found list of all successfully downloaded images.\")\n",
        "else:\n",
        "  needs_labelling = []\n",
        "  print(\"Cannot find Successful_Downloads file for SENTINEL-1 images.\")\n",
        "\n",
        "# Check for successful label file\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "if \"Successful_Labels.txt\" in os.listdir():\n",
        "  # If some labelling has been done, remove the already lebelled images from the list to avoid reprocessing\n",
        "  with open(\"Successful_Labels.txt\", 'r') as file:\n",
        "    already_labelled = file.read().splitlines()\n",
        "  for item in already_labelled:\n",
        "    if item in needs_labelling:\n",
        "      needs_labelling.remove(item)\n",
        "  print(\"Removed all images which were already labelled.\")\n",
        "else:\n",
        "  already_labelled = []\n",
        "  print(\"No images have been labelled yet.\")\n",
        "\n",
        "# Delete the failed labels file so that they can be retried and updated as necessary\n",
        "if \"Failed_Labels.txt\" in os.listdir(): os.remove(\"Failed_Labels.txt\")\n",
        "  \n",
        "if len(needs_labelling) > 0:\n",
        "  print(\"Attempting to label {} remaining images.\\n\".format(len(needs_labelling)))\n",
        "  # Iterate through each image in needs_labelling\n",
        "  for filename in needs_labelling:\n",
        "    try:\n",
        "      # Get the odata info (date, footprint, etc)\n",
        "      year  = filename.split(\"_\")[0]\n",
        "      month = filename.split(\"_\")[1]\n",
        "      day   = filename.split(\"_\")[2]\n",
        "      uuid  = filename.split(\"_\")[3]\n",
        "      odata = search_api.get_product_odata(uuid, full=True)\n",
        "\n",
        "\n",
        "      # Load the resampled image as grayscale\n",
        "      os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "      image = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(year, month, day, uuid)), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      # Load the appropriate concentration map\n",
        "      os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "      nc_file = nc.Dataset(cmems_name.format(year, month, day), 'r', format=\"NETCDF3\")\n",
        "      \n",
        "      # Generate the label from the avaiable information\n",
        "      generate_label(year, month, day, image, odata, nc_file)\n",
        "\n",
        "      # Log the success\n",
        "      os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "      with open(\"Successful_Labels.txt\", 'a+') as f:\n",
        "        f.write(filename + \"\\n\")\n",
        "      clear_output()\n",
        "      print(\"Finished labelling {} of {}: {}\".format(needs_labelling.index(filename) + 1, len(needs_labelling), filename))\n",
        "    except KeyboardInterrupt:\n",
        "      raise\n",
        "    except:\n",
        "      # Log the failure\n",
        "      os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "      with open(\"Failed_Labels.txt\", 'a+') as f:\n",
        "        f.write(filename + \"\\n\")\n",
        "      clear_output()\n",
        "      print(\"Labelling Failed: {}\".format(filename))\n",
        "\n",
        "  clear_output()\n",
        "  print(\"Finished trying to generate all required labels.\")\n",
        "\n",
        "else:\n",
        "  # Needs_labelling is empty\n",
        "  print(\"\\nNo more images need to be labelled.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfcA9sgUq5X0",
        "colab_type": "text"
      },
      "source": [
        "## Delete all labelling patches - Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccESOitEF9ak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if cmems_label_folder in os.listdir():\n",
        "  os.chdir(cmems_label_folder)\n",
        "\n",
        "  if \"Successful_Labels.txt\" in os.listdir(): os.remove(\"Successful_Labels.txt\")\n",
        "  if \"Failed_Labels.txt\" in os.listdir(): os.remove(\"Failed_Labels.txt\")\n",
        "\n",
        "  if \"Concentration Labels\" in os.listdir():\n",
        "    for y in os.listdir(\"Concentration Labels\"):\n",
        "      for m in os.listdir(os.path.join(\"Concentration Labels\", y)):\n",
        "        for d in os.listdir(os.path.join(\"Concentration Labels\", y, m)):\n",
        "          for f in os.listdir(os.path.join(\"Concentration Labels\", y, m, d)):\n",
        "            os.remove(os.path.join(\"Concentration Labels\", y, m, d, f))\n",
        "          os.rmdir(os.path.join(\"Concentration Labels\", y, m, d))\n",
        "        os.rmdir(os.path.join(\"Concentration Labels\", y, m))\n",
        "      os.rmdir(os.path.join(\"Concentration Labels\", y))\n",
        "    os.rmdir(\"Concentration Labels\")\n",
        "\n",
        "  if \"Uncertainty Labels\" in os.listdir():\n",
        "    for y in os.listdir(\"Uncertainty Labels\"):\n",
        "      for m in os.listdir(os.path.join(\"Uncertainty Labels\", y)):\n",
        "        for d in os.listdir(os.path.join(\"Uncertainty Labels\", y, m)):\n",
        "          for f in os.listdir(os.path.join(\"Uncertainty Labels\", y, m, d)):\n",
        "            os.remove(os.path.join(\"Uncertainty Labels\", y, m, d, f))\n",
        "          os.rmdir(os.path.join(\"Uncertainty Labels\", y, m, d))\n",
        "        os.rmdir(os.path.join(\"Uncertainty Labels\", y, m))\n",
        "      os.rmdir(os.path.join(\"Uncertainty Labels\", y))\n",
        "    os.rmdir(\"Uncertainty Labels\")\n",
        "\n",
        "  if len(os.listdir()) == 0:\n",
        "    os.chdir(\"..\")\n",
        "    os.rmdir(cmems_label_folder)\n",
        "\n",
        "  print(\"All files should have been deleted.\")\n",
        "else:\n",
        "  print(\"No CMEMS data to delete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bkZiOEdoKpt",
        "colab_type": "text"
      },
      "source": [
        "# Step 5: Curate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRu9qW3loTAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to decide whether or not to keep a sample\n",
        "def keepSample(date_uuid):\n",
        "  keep = True\n",
        "  # Load the original image, to check the aspect ratio\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "  original = cv2.imread(sentinel_1_quicklook_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]))\n",
        "  aspect_ratio = np.shape(original)[1] / np.shape(original)[0]\n",
        "  if aspect_ratio < 0.8 or aspect_ratio > 1.2: keep = False\n",
        "  \n",
        "  if keep: # don't do unnecessary checks\n",
        "    # Load the concentration label, to check the variance\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "    concentration = cv2.medianBlur(cv2.imread(concentration_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]), cv2.IMREAD_GRAYSCALE), 5) / 100\n",
        "    if np.var(concentration) < 0.05: keep = False\n",
        "  return keep\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "# Check if a list of samples is already there\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if \"Samples.txt\" in os.listdir() and \"Ignored Samples.txt\" in os.listdir():\n",
        "  with open(\"Samples.txt\", 'r') as f:\n",
        "    included_samples = f.read().splitlines()\n",
        "  with open(\"Ignored Samples.txt\", 'r') as f:\n",
        "    ignored_samples = f.read().splitlines()\n",
        "  already_sorted = included_samples + ignored_samples\n",
        "  print(\"Found already sorted samples.\")\n",
        "else:\n",
        "  already_sorted = []\n",
        "  print(\"Nothing previously sorted.\")\n",
        "\n",
        "# Get list of labeled images\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "if \"Successful_Labels.txt\" in os.listdir():\n",
        "  with open(\"Successful_Labels.txt\", 'r') as f:\n",
        "    date_uuids = f.read().splitlines()\n",
        "\n",
        "  # Remove samples already sorted\n",
        "  for date_uuid in already_sorted:\n",
        "    date_uuids.remove(date_uuid)\n",
        "\n",
        "  # Decide which samples to keep\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "  for date_uuid in date_uuids:\n",
        "    clear_output()\n",
        "    print(\"Evaluating sample {} of {}: {}\".format(date_uuids.index(date_uuid) + 1, len(date_uuids), date_uuid))\n",
        "    if keepSample(date_uuid):\n",
        "      os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "      with open(\"Samples.txt\", 'a+') as f:\n",
        "        f.write(date_uuid + \"\\n\")\n",
        "    else:\n",
        "      os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "      with open(\"Ignored Samples.txt\", 'a+') as f:\n",
        "        f.write(date_uuid + \"\\n\")\n",
        "  \n",
        "  # Update the user\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "  print(\"\\nSamples.txt file saved with all included samples.\")\n",
        "  with open(\"Ignored Samples.txt\", 'r') as f:\n",
        "    ignored_samples = f.read().splitlines()\n",
        "  with open(\"Samples.txt\", 'r') as f:\n",
        "    samples = f.read().splitlines()\n",
        "  print(\"{} samples included, out of {} in total.\".format(len(samples), len(samples) + len(ignored_samples)))\n",
        "  print(\"All excluded samples were listed in the Ignored Samples txt file.\")\n",
        "else:\n",
        "  print(\"Successful_Labels.txt could not be found.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMzckDO-4prs",
        "colab_type": "text"
      },
      "source": [
        "### Delete curated sample lists - Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p0uer8A4v7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if \"Samples.txt\" in os.listdir(): os.remove(\"Samples.txt\")\n",
        "if \"Ignored Samples.txt\" in os.listdir(): os.remove(\"Ignored Samples.txt\")\n",
        "\n",
        "print(\"All curated sample lists should have been deleted.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_eoZospiWwc",
        "colab_type": "text"
      },
      "source": [
        "# Step 6: Generate Batch Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgFA0Ax7cS9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Get all existing batch sets\n",
        "# Function to split the dataset and save the txt files\n",
        "def split_dataset():\n",
        "  print(\"Splitting dataset now.\")\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "  \n",
        "  # Read in the 'good' samples, and split for train/test\n",
        "  samples_dataframe      = pd.read_csv(\"Samples.txt\", header=None, names=[\"date_uuid\"])\n",
        "  test_dataframe         = samples_dataframe.sample(frac=0.2, random_state=seed)\n",
        "  train_dataframe        = samples_dataframe.drop(test_dataframe.index)\n",
        "\n",
        "  # Read in the 'bad' samples, and select a number of them for testing later on\n",
        "  test_ignored_dataframe = pd.read_csv(\"Ignored Samples.txt\", header=None, names=[\"date_uuid\"])\n",
        "  test_ignored_dataframe = test_ignored_dataframe.sample(n=min(100, len(test_ignored_dataframe['date_uuid'])), random_state=seed)\n",
        "\n",
        "  # Print a summary\n",
        "  print(\"Dataset has just been split.\\n{} for training, {} for testing, and {} from the ignored samples.\".format(len(train_dataframe['date_uuid']), len(test_dataframe['date_uuid']), len(test_ignored_dataframe['date_uuid'])))\n",
        "\n",
        "  # Save the split lists to file\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\"))\n",
        "  train_dataframe.to_csv(\"Train.txt\", header=False, index=False, mode='w')\n",
        "  test_dataframe.to_csv(\"Test.txt\", header=False, index=False, mode='w')\n",
        "  test_ignored_dataframe.to_csv(\"Test_Ignored.txt\", header=False, index=False, mode='w')\n",
        "  print(\"The split sets have been saved to file for future use.\")\n",
        "\n",
        "# Function to generate and save the image batches\n",
        "def generate_batches(augmentation_args):\n",
        "  # Image preprocessing functions applied at batch time\n",
        "  def image_preprocessing(img):\n",
        "    return img\n",
        "\n",
        "  def concentration_preprocessing(img):\n",
        "    img[:, :, 0] = cv2.medianBlur(img, 5)\n",
        "    return img\n",
        "\n",
        "  def uncertainty_preprocessing(img):\n",
        "    img[:, :, 0] = cv2.medianBlur(img, 5)\n",
        "    return img\n",
        "  \n",
        "  # Add a column for the file paths, generated from the file name\n",
        "  train_dataframe[\"path\"]         = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in train_dataframe['date_uuid']]\n",
        "  test_dataframe[\"path\"]          = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in test_dataframe['date_uuid']]\n",
        "  test_ignored_dataframe[\"path\"]  = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in test_ignored_dataframe['date_uuid']]\n",
        "\n",
        "  # Create DataGenerator objects\n",
        "  train_image_datagen               = ImageDataGenerator(rescale=1./255, **augmentation_args, preprocessing_function=image_preprocessing)\n",
        "  train_label_datagen               = ImageDataGenerator(rescale=1./100, **augmentation_args, preprocessing_function=concentration_preprocessing)\n",
        "  train_uncertainty_datagen         = ImageDataGenerator(rescale=1./100, **augmentation_args, preprocessing_function=uncertainty_preprocessing)\n",
        "\n",
        "  test_image_datagen                = ImageDataGenerator(rescale=1./255, preprocessing_function=image_preprocessing)\n",
        "  test_label_datagen                = ImageDataGenerator(rescale=1./100, preprocessing_function=concentration_preprocessing)\n",
        "  test_uncertainty_datagen          = ImageDataGenerator(rescale=1./100, preprocessing_function=uncertainty_preprocessing)\n",
        "\n",
        "  test_ignored_image_datagen        = ImageDataGenerator(rescale=1./255, preprocessing_function=image_preprocessing)\n",
        "  test_ignored_label_datagen        = ImageDataGenerator(rescale=1./100, preprocessing_function=concentration_preprocessing)\n",
        "  test_ignored_uncertainty_datagen  = ImageDataGenerator(rescale=1./100, preprocessing_function=uncertainty_preprocessing)\n",
        "\n",
        "  # Set the DataGens to flow from the dataframe paths\n",
        "  train_image_generator              =              train_image_datagen.flow_from_dataframe(train_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"rgb\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False,\n",
        "                                                                                            seed=seed)\n",
        "  train_label_generator              =              train_label_datagen.flow_from_dataframe(train_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False,\n",
        "                                                                                            seed=seed)\n",
        "  train_uncertainty_generator        =        train_uncertainty_datagen.flow_from_dataframe(train_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False,\n",
        "                                                                                            seed=seed)\n",
        "  \n",
        "  test_image_generator               =               test_image_datagen.flow_from_dataframe(test_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"rgb\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_label_generator               =               test_label_datagen.flow_from_dataframe(test_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_uncertainty_generator         =         test_uncertainty_datagen.flow_from_dataframe(test_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  \n",
        "  test_ignored_image_generator       =       test_ignored_image_datagen.flow_from_dataframe(test_ignored_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"rgb\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_ignored_label_generator       =       test_ignored_label_datagen.flow_from_dataframe(test_ignored_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_ignored_uncertainty_generator = test_ignored_uncertainty_datagen.flow_from_dataframe(test_ignored_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "\n",
        "  # Update the user\n",
        "  print(\"Finished preparing DataGenerator objects.\")\n",
        "\n",
        "  # Iterate through all the batches and save to file\n",
        "  generators = {\"Train\"        : (train_image_generator,        train_label_generator,        train_uncertainty_generator),\n",
        "                \"Test\"         : (test_image_generator,         test_label_generator,         test_uncertainty_generator),\n",
        "                \"Test_Ignored\" : (test_ignored_image_generator, test_ignored_label_generator, test_ignored_uncertainty_generator)}\n",
        "\n",
        "  # Iterate through each segment (train, val, etc...)\n",
        "  for segment, gen_tuple in generators.items():\n",
        "    # Update the user\n",
        "    print(\"\\nDealing with {} now.\".format(segment))\n",
        "    \n",
        "    # Move into the folder for this segment\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\", batch_name))\n",
        "    if segment not in os.listdir(): os.mkdir(segment)\n",
        "    os.chdir(segment)\n",
        "\n",
        "    # Find the number of digits required to count the number of batches\n",
        "    digits = int(np.ceil(np.log10(len(gen_tuple[0]))))\n",
        "\n",
        "    # Iterate through each batch, and save the batch's data\n",
        "    for batch_number in range(len(gen_tuple[0])):\n",
        "      batch_data = {\"images\"        : np.array(next(gen_tuple[0])),\n",
        "                    \"labels\"        : np.array(next(gen_tuple[1]), ),\n",
        "                    \"uncertainties\" : np.array(next(gen_tuple[2]))}\n",
        "      filename = (\"{:0\" + str(digits) +  \"}.npz\").format(batch_number)\n",
        "      if filename not in os.listdir():\n",
        "        np.savez(filename, **batch_data)\n",
        "        print(\"Saved batch {} of {}\".format(batch_number + 1, len(gen_tuple[0])))\n",
        "      else:\n",
        "        print(\"Batch already saved: {} of {}\".format(batch_number + 1, len(gen_tuple[0])))\n",
        "\n",
        "  print(\"\\nFinished saving batches.\")\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "# Names for each batch segemnt\n",
        "batch_segments = [\"Train\", \"Test\", \"Test_Ignored\"]\n",
        "\n",
        "# Set data augmentation parameters\n",
        "augmentation_args = {\"rotation_range\" : 10,\n",
        "                     \"fill_mode\"        : \"constant\",\n",
        "                     \"cval\"             : 0,\n",
        "                     \"horizontal_flip\"  : True,\n",
        "                     \"vertical_flip\"    : True}\n",
        "\n",
        "# Get list of available batch sets\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if \"Batches\" in os.listdir():\n",
        "  os.chdir(\"Batches\")\n",
        "  if len(os.listdir()) > len(batch_segments):\n",
        "    print(\"Here is a list of all available batch sets:\")\n",
        "    for d in os.listdir():\n",
        "      if os.path.isdir(d): print(\"\\t\" + d)\n",
        "    print(\"Please choose one and enter its name into the following cell. Otherwise create a new batch set below.\")\n",
        "  else:\n",
        "    print(\"No batches available. Please create a new batch set below.\")\n",
        "else:\n",
        "  os.mkdir(\"Batches\")\n",
        "  split_dataset()\n",
        "  print(\"\\nNo batches available. Please create a new batch set below.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WF51geiXqXf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Create new batch set\n",
        "batch_name = \"\" #@param {type:\"string\"}\n",
        "use_augmentation = True #@param {type:\"boolean\"}\n",
        "if use_augmentation: batch_name += \"_A\"\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "# Check that the batch name is unique\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\"))\n",
        "if batch_name in os.listdir():\n",
        "  print(\"This batch set name already exists. Please chose a unique name if you want to create a new batch set.\")\n",
        "else:\n",
        "  # Check if all three split files are here\n",
        "  if not all([\"{}.txt\".format(s) in os.listdir() for s in batch_segments]):\n",
        "    # Missing split files. Make them\n",
        "    print(\"Warning: One or more of the split files was missing. It will be generated now.\")\n",
        "    split_dataset()\n",
        "  else:\n",
        "    print(\"Dataset has already been split. To re-split, delete the txt files and run this code again.\")\n",
        "  \n",
        "  # Load the dataset splits\n",
        "  train_dataframe        = pd.read_csv(\"Train.txt\", header=None, names=['date_uuid'])\n",
        "  test_dataframe         = pd.read_csv(\"Test.txt\", header=None, names=['date_uuid'])\n",
        "  test_ignored_dataframe = pd.read_csv(\"Test_Ignored.txt\", header=None, names=['date_uuid'])\n",
        "  \n",
        "  # Generate the batch set\n",
        "  os.mkdir(batch_name)\n",
        "  generate_batches(augmentation_args if use_augmentation else {})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8bejtKmJ3rm",
        "colab_type": "text"
      },
      "source": [
        "# Validation Procedures (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqHHQs3txiSZ",
        "colab_type": "text"
      },
      "source": [
        "## Patch Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB-_GH6vxphV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create directory for Validation data\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if validations_folder not in os.listdir(): os.mkdir(validations_folder)\n",
        "\n",
        "# Flag for everything being ready to make the comparison\n",
        "ok_flag = True\n",
        "\n",
        "# Load list of sentinel images\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "if \"Successful_Downloads.txt\" in os.listdir():\n",
        "  with open(\"Successful_Downloads.txt\") as f:\n",
        "    successful_downloads = f.read().splitlines()\n",
        "else:\n",
        "  print(\"No Image Downloads!\")\n",
        "  ok_flag = False\n",
        "\n",
        "# Load list of labels completed\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "if \"Successful_Labels.txt\" in os.listdir():\n",
        "  with open(\"Successful_Labels.txt\") as f:\n",
        "    successful_labels = f.read().splitlines()\n",
        "else:\n",
        "  print(\"No Labels Generated!\")\n",
        "  ok_flag = False\n",
        "\n",
        "# Check that every image has a label\n",
        "if ok_flag:\n",
        "  for filename in successful_downloads:\n",
        "    if filename not in successful_labels:\n",
        "      ok_flag = False\n",
        "      break\n",
        "\n",
        "# Make the comparison if everyhting is ok so far\n",
        "if ok_flag:\n",
        "  # Read in the list of ids which have already been processed\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, validations_folder))\n",
        "  already_processed = []\n",
        "  if \"Patch_Label_Validations.txt\" in os.listdir():\n",
        "    with open(\"Patch_Label_Validations.txt\", 'r') as f:\n",
        "      already_processed = f.read().splitlines()\n",
        "  \n",
        "  # Only process ids which have not yet been processed\n",
        "  needs_processing = []\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "  if \"Samples.txt\" in os.listdir():\n",
        "    with open(\"Samples.txt\", 'r') as f:\n",
        "      successful_downloads = f.read().splitlines()\n",
        "  for filename in successful_downloads:\n",
        "    if filename not in already_processed:\n",
        "      needs_processing.append(filename)\n",
        "\n",
        "  # Process each id\n",
        "  for filename in needs_processing[:20]:\n",
        "    # Get date information\n",
        "    y = filename.split(\"_\")[0]\n",
        "    m = filename.split(\"_\")[1]\n",
        "    d = filename.split(\"_\")[2]\n",
        "    uuid = filename.split(\"_\")[3]\n",
        "\n",
        "    # Load the image\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "    # full_image  = cv2.cvtColor(cv2.imread(sentinel_1_quicklook_name.format(y, m, d, uuid)), cv2.COLOR_BGR2RGB)\n",
        "    small_image = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(y, m, d, uuid)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Load the labels\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "    concentration = cv2.imread(concentration_name.format(y, m, d, uuid), cv2.IMREAD_GRAYSCALE)\n",
        "    uncertainty   = cv2.imread(uncertainty_name.format(y, m, d, uuid), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Plot\n",
        "    plt.gcf().set_size_inches(20,5)\n",
        "    plt.suptitle(filename)\n",
        "    # plt.subplot(1, 4, 1)\n",
        "    # plt.imshow(full_image)\n",
        "    # plt.title(\"Original SAR Image\")\n",
        "    # plt.axis('off')\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(small_image)\n",
        "    plt.title(\"Resampled SAR Image\")\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(concentration, cmap='hot', vmin=0, vmax=100)\n",
        "    plt.title(\"Concentration Patch Label\")\n",
        "    plt.axis('off')\n",
        "    plt.colorbar()\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(uncertainty, cmap='hot', vmin=0, vmax=100)\n",
        "    plt.title(\"Uncertainty Patch Label\")\n",
        "    plt.axis('off')\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Save to Drive\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name, validations_folder))\n",
        "    if not os.path.exists(\"Patch Label Validation/{}/{}/{}\".format(y, m, d)): os.makedirs(\"Patch Label Validation/{}/{}/{}\".format(y, m, d))\n",
        "    plt.savefig(label_comparison_name.format(y, m, d, uuid), bbox_inches=0)\n",
        "    # clear_output()\n",
        "    plt.show()\n",
        "\n",
        "    odata = search_api.get_product_odata(uuid, full=True)\n",
        "    size = get_height_width(odata)\n",
        "    print(\"Done saving file {} of {}. Pixel Spacing (rg x az): {:.2f}km x {:.2f}km\".format(needs_processing.index(filename) + 1, len(needs_processing), size[1] / image_dimension, size[0] / image_dimension))\n",
        "\n",
        "    # Save the id to the text file for progress tracking\n",
        "    with open(\"Patch_Label_Validations.txt\", 'a+') as f:\n",
        "      f.write(filename + \"\\n\")\n",
        "else:\n",
        "  print(\"'ok_flag' tripped. Check that all previous steps have been completed correctly.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IBNvazrxpsV",
        "colab_type": "text"
      },
      "source": [
        "### Delete all patch comparisons - Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfuTmKdQxp0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if validations_folder in os.listdir():\n",
        "  os.chdir(validations_folder)\n",
        "\n",
        "  if \"Patch_Label_Validations.txt\" in os.listdir(): os.remove(\"Patch_Label_Validations.txt\")\n",
        "\n",
        "  if \"Patch Label Validation\" in os.listdir():\n",
        "    for y in os.listdir(\"Patch Label Validation\"):\n",
        "      for m in os.listdir(os.path.join(\"Patch Label Validation\", y)):\n",
        "        for d in os.listdir(os.path.join(\"Patch Label Validation\", y, m)):\n",
        "          for f in os.listdir(os.path.join(\"Patch Label Validation\", y, m, d)):\n",
        "            os.remove(os.path.join(\"Patch Label Validation\", y, m, d, f))\n",
        "          os.rmdir(os.path.join(\"Patch Label Validation\", y, m, d))\n",
        "        os.rmdir(os.path.join(\"Patch Label Validation\", y, m))\n",
        "      os.rmdir(os.path.join(\"Patch Label Validation\", y))\n",
        "    os.rmdir(\"Patch Label Validation\")\n",
        "\n",
        "  if len(os.listdir()) == 0:\n",
        "    os.chdir(\"..\")\n",
        "    os.rmdir(validations_folder)\n",
        "\n",
        "  print(\"All files should have been deleted.\")\n",
        "else:\n",
        "  print(\"No validation data to delete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An-u-PqxxEq1",
        "colab_type": "text"
      },
      "source": [
        "## Interpolation Validation (Only used with South Sandwich Islands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DncRBi5qfB0R",
        "colab_type": "text"
      },
      "source": [
        "**Validate interpolation methods by masking each island as an accuracy check**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7hxe0IzHTxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_height_width(odata):\n",
        "  # Get footprint for interpolation\n",
        "  footprint = odata['footprint'][9:-2].split(\",\")\n",
        "  top_left_lat     = float(footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2].split(\" \")[1])\n",
        "  top_left_lon     = float(footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2].split(\" \")[0])\n",
        "  top_right_lat    = float(footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3].split(\" \")[1])\n",
        "  top_right_lon    = float(footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3].split(\" \")[0])\n",
        "  bottom_right_lat = float(footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0].split(\" \")[1])\n",
        "  bottom_right_lon = float(footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0].split(\" \")[0])\n",
        "  bottom_left_lat  = float(footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1].split(\" \")[1])\n",
        "  bottom_left_lon  = float(footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1].split(\" \")[0])\n",
        "  \n",
        "  # Get coordinate pairs for each corner\n",
        "  top_left     = (top_left_lat, top_left_lon)\n",
        "  top_right    = (top_right_lat, top_right_lon)\n",
        "  bottom_left  = (bottom_left_lat, bottom_left_lon)\n",
        "  bottom_right = (bottom_right_lat, bottom_right_lon)\n",
        "  \n",
        "  # Assume the image is rectangular, and calculate the dimensions\n",
        "  height = (geoDist.distance(top_left, bottom_left).km + geoDist.distance(top_right, bottom_right).km) / 2.0\n",
        "  width  = (geoDist.distance(top_left, top_right).km + geoDist.distance(bottom_left, bottom_right).km) / 2.0\n",
        "  \n",
        "  return (height, width)\n",
        "\n",
        "def generate_mask(odata):\n",
        "  # Target coordinates\n",
        "  # This is for the following dataset name: \"IslandsInterpolation_{}_{}\".format(start_date, end_date)\n",
        "  islands = [(-56.71, -27.18),\n",
        "             (-57.10, -26.70),\n",
        "             (-57.80, -26.49),\n",
        "             (-58.43, -26.38),\n",
        "             (-59.04, -26.54),\n",
        "             (-59.46, -27.26)]\n",
        "  radius = 10 #km\n",
        "  \n",
        "  # Mask array\n",
        "  mask = np.zeros((image_dimension, image_dimension), dtype=np.uint8)\n",
        "  \n",
        "  # get footprint for interpolation\n",
        "  footprint = odata['footprint'][9:-2].split(\",\")\n",
        "  top_left_lat     = float(footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2].split(\" \")[1])\n",
        "  top_left_lon     = float(footprint[0 if odata['Pass direction'] == \"ASCENDING\" else 2].split(\" \")[0])\n",
        "  top_right_lat    = float(footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3].split(\" \")[1])\n",
        "  top_right_lon    = float(footprint[1 if odata['Pass direction'] == \"ASCENDING\" else 3].split(\" \")[0])\n",
        "  bottom_right_lat = float(footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0].split(\" \")[1])\n",
        "  bottom_right_lon = float(footprint[2 if odata['Pass direction'] == \"ASCENDING\" else 0].split(\" \")[0])\n",
        "  bottom_left_lat  = float(footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1].split(\" \")[1])\n",
        "  bottom_left_lon  = float(footprint[3 if odata['Pass direction'] == \"ASCENDING\" else 1].split(\" \")[0])\n",
        "  \n",
        "  left_edge  = [np.linspace(top_left_lat,  bottom_left_lat,  image_dimension), np.linspace(top_left_lon,  bottom_left_lon,  image_dimension)]\n",
        "  right_edge = [np.linspace(top_right_lat, bottom_right_lat, image_dimension), np.linspace(top_right_lon, bottom_right_lon, image_dimension)]\n",
        "  for y_pixel in range(image_dimension):\n",
        "    interpolated_row = [np.linspace(left_edge[0][y_pixel], right_edge[0][y_pixel], image_dimension), np.linspace(left_edge[1][y_pixel], right_edge[1][y_pixel], image_dimension)]\n",
        "    for x_pixel in range(image_dimension):\n",
        "      pixel_lat = interpolated_row[0][x_pixel]\n",
        "      pixel_lon = interpolated_row[1][x_pixel]\n",
        "      for island in islands:\n",
        "        if geoDist.distance((pixel_lat, pixel_lon), island).km < radius:\n",
        "          # This pixel is in the desired range\n",
        "          mask[y_pixel][x_pixel] = 1\n",
        "  return mask\n",
        "\n",
        "def show_mask_validation(odata):\n",
        "  # Image mask generation\n",
        "  mask = generate_mask(odata)\n",
        "  size = get_height_width(odata)\n",
        "\n",
        "  # Resampling and masking\n",
        "  date = odata['date']\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "  full_img   = cv2.cvtColor(cv2.imread(sentinel_1_quicklook_name.format(date.year, date.month, date.day, odata['id'])), cv2.COLOR_BGR2RGB)\n",
        "  small_img  = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(date.year, date.month, date.day, odata['id'])), cv2.COLOR_BGR2RGB)\n",
        "  masked_img = cv2.bitwise_and(small_img, small_img, mask=mask)\n",
        "\n",
        "  # Show the results\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.suptitle(sentinel_1_quicklook_name.format(date.year, date.month, date.day, odata['id']).split(\"/\")[-1][:-4])\n",
        "  plt.subplot(1,4,1)\n",
        "  # plt.title(\"H: {:.1f}km, W: {:.1f}km\".format(size[0], size[1]))\n",
        "  plt.title(\"Original SAR Image\")\n",
        "  plt.imshow(full_img)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1,4,2)\n",
        "  # plt.title(odata['Pass direction'])\n",
        "  plt.title(\"Resampled SAR Image\")\n",
        "  plt.imshow(small_img)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1,4,3)\n",
        "  # plt.title(\"Pixel Spacing (rg x az): {:.2f}km x {:.2f}km\".format(size[1] / image_dimension, size[0] / image_dimension))\n",
        "  plt.title(\"Proximity Mask (r=10km)\")\n",
        "  plt.imshow(mask)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1,4,4)\n",
        "  plt.title(\"Mask Applied\")\n",
        "  plt.imshow(masked_img)\n",
        "  plt.axis('off')\n",
        "  if 1 in mask:\n",
        "    # Only save figures where the islands are actually included. Others aren't useful for this test.\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "    if validations_folder not in os.listdir(): os.mkdir(validations_folder)\n",
        "    os.chdir(validations_folder)\n",
        "    if not os.path.exists(\"Interpolation Validation/{}/{}/{}\".format(date.year, date.month, date.day)): os.makedirs(\"Interpolation Validation/{}/{}/{}\".format(date.year, date.month, date.day))\n",
        "    plt.savefig(interpolation_name.format(date.year, date.month, date.day, odata['id']), bbox_inches=0)\n",
        "  plt.show()\n",
        "\n",
        "# Compute mask for all images, to evaluate the accuracy of the interpolation\n",
        "# Create directory for Validation data\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if validations_folder not in os.listdir(): os.mkdir(validations_folder)\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "with open(\"Successful_Downloads.txt\") as f:\n",
        "  uuids = [filename.split(\"_\")[-1] for filename in f.read().splitlines()]\n",
        "for uuid in uuids:\n",
        "  odata = search_api.get_product_odata(uuid, full=True)\n",
        "  show_mask_validation(odata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU47KhY10kt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a txt file for successful interpolation validations\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, validations_folder))\n",
        "for y in os.listdir(\"Interpolation Validation\"):\n",
        "  for m in os.listdir(os.path.join(\"Interpolation Validation\", y)):\n",
        "    for d in os.listdir(os.path.join(\"Interpolation Validation\", y, m)):\n",
        "      for f in os.listdir(os.path.join(\"Interpolation Validation\", y, m, d)):\n",
        "        with open(\"Interpolation Validations.txt\", 'a+') as text_file:\n",
        "          text_file.write(f.split(\".\")[0] + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzIKnpYB_Qyc",
        "colab_type": "text"
      },
      "source": [
        "### Delete all interpolation validation results- Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqWwD2F7C7fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if validations_folder in os.listdir():\n",
        "  os.chdir(validations_folder)\n",
        "\n",
        "  if \"Interpolation Validations.txt\" in os.listdir(): os.remove(\"Interpolation Validations.txt\")\n",
        "\n",
        "  if \"Interpolation Validation\" in os.listdir():\n",
        "    for y in os.listdir(\"Interpolation Validation\"):\n",
        "      for m in os.listdir(os.path.join(\"Interpolation Validation\", y)):\n",
        "        for d in os.listdir(os.path.join(\"Interpolation Validation\", y, m)):\n",
        "          for f in os.listdir(os.path.join(\"Interpolation Validation\", y, m, d)):\n",
        "            os.remove(os.path.join(\"Interpolation Validation\", y, m, d, f))\n",
        "          os.rmdir(os.path.join(\"Interpolation Validation\", y, m, d))\n",
        "        os.rmdir(os.path.join(\"Interpolation Validation\", y, m))\n",
        "      os.rmdir(os.path.join(\"Interpolation Validation\", y))\n",
        "    os.rmdir(\"Interpolation Validation\")\n",
        "\n",
        "  if len(os.listdir()) == 0:\n",
        "    os.chdir(\"..\")\n",
        "    os.rmdir(validations_folder)\n",
        "\n",
        "  print(\"All files should have been deleted.\")\n",
        "else:\n",
        "  print(\"No validation data to delete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}