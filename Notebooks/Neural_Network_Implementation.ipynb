{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network Implementation",
      "provenance": [],
      "collapsed_sections": [
        "p-iUeygYXdl7",
        "RZwClSC7lFnv",
        "_aExZsqoHX1v",
        "bC8i7uM_ZXR9",
        "wuytcVdQ05A0",
        "IbdAuG-gAN2c",
        "HSPaTXK6A0-N",
        "fJptAE-M1a3o",
        "UvVQHjdwLEix",
        "xZNZT4N01tXy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys2zprSMvnqt",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Run to Setup\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import Sequence\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Mount my google drive\n",
        "from google.colab import drive\n",
        "clear_output()\n",
        "print(\"Mount Google Drive Now\")\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Dataset file structure information\n",
        "sentinel_1_folder       = \"SENTINEL-1 SAR\"\n",
        "cmems_label_folder      = \"CMEMS Concentration\"\n",
        "validations_folder      = \"Image Processing Validation\"\n",
        "\n",
        "# Input image path information\n",
        "# (Only use when in the \"SENTINEL-1 SAR\" folder)\n",
        "sentinel_1_quicklook_name = \"Quick Look Images/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "sentinel_1_resampled_name = \"Resampled Images/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "\n",
        "# Label image path information\n",
        "# (Only use when in the \"CMEMS Boundary\" folder)\n",
        "concentration_name = \"Concentration Labels/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "uncertainty_name   = \"Uncertainty Labels/{0}/{1}/{2}/{0}_{1}_{2}_{3}.png\" # 0:YEAR, 1:MONTH, 2:DAY, 3:UUID\n",
        "\n",
        "# Models folder location\n",
        "model_location_path = \"/content/gdrive/Shared drives/ICE_CHARTING_UCT_WSA/Models\"\n",
        "\n",
        "# Dimension of all input images and labels (assume nxn)\n",
        "image_dimension = 128\n",
        "\n",
        "# Batch size for training and testing\n",
        "batch_size = 16\n",
        "seed = 1\n",
        "\n",
        "# Show the available datasets\n",
        "clear_output()\n",
        "dataset_location_path   = \"/content/gdrive/Shared drives/ICE_CHARTING_UCT_WSA/Data\"\n",
        "print(\"The following datasets are available:\")\n",
        "for name in os.listdir(dataset_location_path):\n",
        "  print(\"\\t\" + name)\n",
        "print(\"Please choose one and use it in the following cell.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29KEOICB0VZd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Please enter the dataset name\n",
        "dataset_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Check if the dataset exists\n",
        "os.chdir(dataset_location_path)\n",
        "if dataset_name  in os.listdir():\n",
        "  print(\"Dataset found! OK to continue.\")\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "else:\n",
        "  print(\"Dataset does not exist. Please check the name and dates, then try again.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-iUeygYXdl7",
        "colab_type": "text"
      },
      "source": [
        "# Batch Set Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgFA0Ax7cS9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Get all existing batch sets\n",
        "# Function to split the dataset and save the txt files\n",
        "def split_dataset():\n",
        "  print(\"Splitting dataset now.\")\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "  \n",
        "  # Read in the 'good' samples, and split for train/test\n",
        "  samples_dataframe      = pd.read_csv(\"Samples.txt\", header=None, names=[\"date_uuid\"])\n",
        "  test_dataframe         = samples_dataframe.sample(frac=0.2, random_state=seed)\n",
        "  train_dataframe        = samples_dataframe.drop(test_dataframe.index)\n",
        "\n",
        "  # Read in the 'bad' samples, and select a number of them for testing later on\n",
        "  test_ignored_dataframe = pd.read_csv(\"Ignored Samples.txt\", header=None, names=[\"date_uuid\"])\n",
        "  test_ignored_dataframe = test_ignored_dataframe.sample(n=min(100, len(test_ignored_dataframe['date_uuid'])), random_state=seed)\n",
        "\n",
        "  # Print a summary\n",
        "  print(\"Dataset has just been split.\\n{} for training, {} for testing, and {} from the ignored samples.\".format(len(train_dataframe['date_uuid']), len(test_dataframe['date_uuid']), len(test_ignored_dataframe['date_uuid'])))\n",
        "\n",
        "  # Save the split lists to file\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\"))\n",
        "  train_dataframe.to_csv(\"Train.txt\", header=False, index=False, mode='w')\n",
        "  test_dataframe.to_csv(\"Test.txt\", header=False, index=False, mode='w')\n",
        "  test_ignored_dataframe.to_csv(\"Test_Ignored.txt\", header=False, index=False, mode='w')\n",
        "  print(\"The split sets have been saved to file for future use.\")\n",
        "\n",
        "# Function to generate and save the image batches\n",
        "def generate_batches(augmentation_args):\n",
        "  # Image preprocessing functions applied at batch time\n",
        "  def image_preprocessing(img):\n",
        "    return img\n",
        "\n",
        "  def concentration_preprocessing(img):\n",
        "    img[:, :, 0] = cv2.medianBlur(img, 5)\n",
        "    return img\n",
        "\n",
        "  def uncertainty_preprocessing(img):\n",
        "    img[:, :, 0] = cv2.medianBlur(img, 5)\n",
        "    return img\n",
        "  \n",
        "  # Add a column for the file paths, generated from the file name\n",
        "  train_dataframe[\"path\"]         = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in train_dataframe['date_uuid']]\n",
        "  test_dataframe[\"path\"]          = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in test_dataframe['date_uuid']]\n",
        "  test_ignored_dataframe[\"path\"]  = [\"{}/{}/{}/{}.png\".format(name.split(\"_\")[0], name.split(\"_\")[1], name.split(\"_\")[2], name) for name in test_ignored_dataframe['date_uuid']]\n",
        "\n",
        "  # Create DataGenerator objects\n",
        "  train_image_datagen               = ImageDataGenerator(rescale=1./255, **augmentation_args, preprocessing_function=image_preprocessing)\n",
        "  train_label_datagen               = ImageDataGenerator(rescale=1./100, **augmentation_args, preprocessing_function=concentration_preprocessing)\n",
        "  train_uncertainty_datagen         = ImageDataGenerator(rescale=1./100, **augmentation_args, preprocessing_function=uncertainty_preprocessing)\n",
        "\n",
        "  test_image_datagen                = ImageDataGenerator(rescale=1./255, preprocessing_function=image_preprocessing)\n",
        "  test_label_datagen                = ImageDataGenerator(rescale=1./100, preprocessing_function=concentration_preprocessing)\n",
        "  test_uncertainty_datagen          = ImageDataGenerator(rescale=1./100, preprocessing_function=uncertainty_preprocessing)\n",
        "\n",
        "  test_ignored_image_datagen        = ImageDataGenerator(rescale=1./255, preprocessing_function=image_preprocessing)\n",
        "  test_ignored_label_datagen        = ImageDataGenerator(rescale=1./100, preprocessing_function=concentration_preprocessing)\n",
        "  test_ignored_uncertainty_datagen  = ImageDataGenerator(rescale=1./100, preprocessing_function=uncertainty_preprocessing)\n",
        "\n",
        "  # Set the DataGens to flow from the dataframe paths\n",
        "  train_image_generator              =              train_image_datagen.flow_from_dataframe(train_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"rgb\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False,\n",
        "                                                                                            seed=seed)\n",
        "  train_label_generator              =              train_label_datagen.flow_from_dataframe(train_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False,\n",
        "                                                                                            seed=seed)\n",
        "  train_uncertainty_generator        =        train_uncertainty_datagen.flow_from_dataframe(train_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False,\n",
        "                                                                                            seed=seed)\n",
        "  \n",
        "  test_image_generator               =               test_image_datagen.flow_from_dataframe(test_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"rgb\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_label_generator               =               test_label_datagen.flow_from_dataframe(test_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_uncertainty_generator         =         test_uncertainty_datagen.flow_from_dataframe(test_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  \n",
        "  test_ignored_image_generator       =       test_ignored_image_datagen.flow_from_dataframe(test_ignored_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, sentinel_1_folder, \"Resampled Images\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"rgb\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_ignored_label_generator       =       test_ignored_label_datagen.flow_from_dataframe(test_ignored_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Concentration Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "  test_ignored_uncertainty_generator = test_ignored_uncertainty_datagen.flow_from_dataframe(test_ignored_dataframe,\n",
        "                                                                                            directory=os.path.join(dataset_location_path, dataset_name, cmems_label_folder, \"Uncertainty Labels\"),\n",
        "                                                                                            x_col=\"path\",\n",
        "                                                                                            target_size=(image_dimension, image_dimension),\n",
        "                                                                                            color_mode=\"grayscale\",\n",
        "                                                                                            class_mode=None,\n",
        "                                                                                            batch_size=batch_size,\n",
        "                                                                                            shuffle=False)\n",
        "\n",
        "  # Update the user\n",
        "  print(\"Finished preparing DataGenerator objects.\")\n",
        "\n",
        "  # Iterate through all the batches and save to file\n",
        "  generators = {\"Train\"        : (train_image_generator,        train_label_generator,        train_uncertainty_generator),\n",
        "                \"Test\"         : (test_image_generator,         test_label_generator,         test_uncertainty_generator),\n",
        "                \"Test_Ignored\" : (test_ignored_image_generator, test_ignored_label_generator, test_ignored_uncertainty_generator)}\n",
        "\n",
        "  # Iterate through each segment (train, val, etc...)\n",
        "  for segment, gen_tuple in generators.items():\n",
        "    # Update the user\n",
        "    print(\"\\nDealing with {} now.\".format(segment))\n",
        "    \n",
        "    # Move into the folder for this segment\n",
        "    os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\", batch_name))\n",
        "    if segment not in os.listdir(): os.mkdir(segment)\n",
        "    os.chdir(segment)\n",
        "\n",
        "    # Find the number of digits required to count the number of batches\n",
        "    digits = int(np.ceil(np.log10(len(gen_tuple[0]))))\n",
        "\n",
        "    # Iterate through each batch, and save the batch's data\n",
        "    for batch_number in range(len(gen_tuple[0])):\n",
        "      batch_data = {\"images\"        : np.array(next(gen_tuple[0])),\n",
        "                    \"labels\"        : np.array(next(gen_tuple[1]), ),\n",
        "                    \"uncertainties\" : np.array(next(gen_tuple[2]))}\n",
        "      filename = (\"{:0\" + str(digits) +  \"}.npz\").format(batch_number)\n",
        "      if filename not in os.listdir():\n",
        "        np.savez(filename, **batch_data)\n",
        "        print(\"Saved batch {} of {}\".format(batch_number + 1, len(gen_tuple[0])))\n",
        "      else:\n",
        "        print(\"Batch already saved: {} of {}\".format(batch_number + 1, len(gen_tuple[0])))\n",
        "\n",
        "  print(\"\\nFinished saving batches.\")\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "# Names for each batch segemnt\n",
        "batch_segments = [\"Train\", \"Test\", \"Test_Ignored\"]\n",
        "\n",
        "# Set data augmentation parameters\n",
        "augmentation_args = {\"rotation_range\" : 10,\n",
        "                     \"fill_mode\"        : \"constant\",\n",
        "                     \"cval\"             : 0,\n",
        "                     \"horizontal_flip\"  : True,\n",
        "                     \"vertical_flip\"    : True}\n",
        "\n",
        "# Get list of available batch sets\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "if \"Batches\" in os.listdir():\n",
        "  os.chdir(\"Batches\")\n",
        "  # Load the dataset splits\n",
        "  train_dataframe        = pd.read_csv(\"Train.txt\", header=None, names=['date_uuid'])\n",
        "  test_dataframe         = pd.read_csv(\"Test.txt\", header=None, names=['date_uuid'])\n",
        "  test_ignored_dataframe = pd.read_csv(\"Test_Ignored.txt\", header=None, names=['date_uuid'])\n",
        "\n",
        "  # Show all available batch sets\n",
        "  if len(os.listdir()) > len(batch_segments):\n",
        "    print(\"Here is a list of all available batch sets:\")\n",
        "    for d in os.listdir():\n",
        "      if os.path.isdir(d): print(\"\\t\" + d)\n",
        "    print(\"Please choose one and enter its name into the following cell. Otherwise create a new batch set below.\")\n",
        "  else:\n",
        "    print(\"No batch sets have been made yet.\")\n",
        "else:\n",
        "  print(\"No batch sets have been made yet.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtAotNFe79vZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Load an existing batch set\n",
        "batch_name = \"\" #@param {type:\"string\"}\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, \"Batches\"))\n",
        "if batch_name in os.listdir():\n",
        "  os.chdir(batch_name)\n",
        "  # Check if the batch set has been completed\n",
        "  if all([f in os.listdir() for f in batch_segments]):\n",
        "    # All batch folders are present. Now check their contents\n",
        "    number_of_batches = [(int) (np.ceil(len(df['date_uuid']) / batch_size)) for df in [train_dataframe, test_dataframe, test_ignored_dataframe]]\n",
        "    if not all([len(os.listdir(s)) == number_of_batches[i] for i, s in enumerate(batch_segments)]):\n",
        "      print(\"This batch set is incomplete. It will be completed now.\")\n",
        "      generate_batches(augmentation_args if batch_name.split(\"_\")[-1] == \"A\" else {})\n",
        "    else:\n",
        "      print(\"Batch set found. OK to continue.\")\n",
        "  else:\n",
        "    print(\"This batch set is incomplete. It will be completed now.\")\n",
        "    generate_batches(augmentation_args if batch_name.split(\"_\")[-1] == \"A\" else {})\n",
        "else:\n",
        "  print(\"Cannot find batch set. Please use a name that already exists.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th_OAjcFABA3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Create Sequence Feeder Objects\n",
        "class dataSequence(Sequence):\n",
        "\n",
        "  def __init__(self, split, batch_name=batch_name, rg_only=True, predict=False):\n",
        "    self.split          = split\n",
        "    self.dataframe      = pd.read_csv(os.path.join(dataset_location_path, dataset_name, \"Batches\", \"{}.txt\".format(self.split)), header=None, names=['date_uuid'])\n",
        "    self.batch_name     = batch_name\n",
        "    self.batches        = sorted(os.listdir(os.path.join(dataset_location_path, dataset_name, \"Batches\", self.batch_name, self.split)))\n",
        "    self.rg_only        = rg_only\n",
        "    self.predict        = predict\n",
        "\n",
        "  # Used by keras to get the number of batches in this segment\n",
        "  def __len__(self):\n",
        "    return len(self.batches)\n",
        "  \n",
        "  # Used by keras to parse batches of input to the model\n",
        "  def __getitem__(self, index):\n",
        "    self.batch = np.load(os.path.join(dataset_location_path, dataset_name, \"Batches\", self.batch_name, self.split, self.batches[index]))\n",
        "    if self.predict:\n",
        "      # Return only the input images for prediction\n",
        "      return self.batch['images'][:, :, :, :2] if self.rg_only else self.batch['images']\n",
        "    else:\n",
        "      # Return image, label, and uncertainty data\n",
        "      return (self.batch['images'][:, :, :, :2] if self.rg_only else self.batch['images'], np.concatenate((self.batch['labels'], self.batch['uncertainties']), axis=-1))\n",
        "\n",
        "  # Used for plotting (not by keras)\n",
        "  def getImageBatch(self, index):\n",
        "    self.batch = np.load(os.path.join(dataset_location_path, dataset_name, \"Batches\", self.batch_name, self.split, self.batches[index]))['images']\n",
        "    if self.rg_only: self.batch[:, :, :, 2] = np.zeros(np.shape(self.batch), dtype=self.batch.dtype)[:, :, :, 2]\n",
        "    return self.batch\n",
        "  \n",
        "  # Used for plotting (not by keras)\n",
        "  def getLabelBatch(self, index):\n",
        "    return np.load(os.path.join(dataset_location_path, dataset_name, \"Batches\", self.batch_name, self.split, self.batches[index]))['labels']\n",
        "\n",
        "  # Used for plotting (not by keras)\n",
        "  def getUncertaintyBatch(self, index):\n",
        "    return np.load(os.path.join(dataset_location_path, dataset_name, \"Batches\", self.batch_name, self.split, self.batches[index]))['uncertainties']\n",
        "\n",
        "  # Used for keeping track of specific inputs while plotting\n",
        "  def getDateUUID(self, index):\n",
        "    return [self.dataframe['date_uuid'][i] for i in range(index * batch_size, min((index + 1) * batch_size, len(self.dataframe['date_uuid'])))]\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "train_sequence        = dataSequence(\"Train\")\n",
        "test_sequence         = dataSequence(\"Test\")\n",
        "predict_sequence      = dataSequence(\"Test\", predict=True)\n",
        "test_ignored_sequence = dataSequence(\"Test_Ignored\")\n",
        "\n",
        "print(\"Sequence objects ready.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwClSC7lFnv",
        "colab_type": "text"
      },
      "source": [
        "## Sequence Output Validation (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzUdDExANLaM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Choose which sequence to validate\n",
        "seq = train_sequence #@param [\"train_sequence\", \"test_sequence\", \"predict_sequence\", \"test_ignored_sequence\"] {type:\"raw\"}\n",
        "\n",
        "for i in range(len(seq)):\n",
        "  images = seq.getImageBatch(i)\n",
        "  labels = seq.getLabelBatch(i)\n",
        "  uncert = seq.getUncertaintyBatch(i)\n",
        "  uuids  = seq.getDateUUID(i)\n",
        "  for j in range(len(images)):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.suptitle(uuids[j], fontsize=18)\n",
        "    \n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(images[j])\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(labels[j][:, :, 0], cmap=\"hot\", vmin=0, vmax=1)\n",
        "    plt.title(\"Concentration Label\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(uncert[j][:, :, 0], cmap=\"hot\", vmin=0, vmax=1)\n",
        "    plt.title(\"Concentration Uncertainty\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aExZsqoHX1v",
        "colab_type": "text"
      },
      "source": [
        "## Filter Testing (for preprocessing of Images, Concentration & Uncertainty)  (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5fJWZ66tBth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_size = 5\n",
        "\n",
        "# Define some filter functions\n",
        "def median_filter(img, size=kernel_size):\n",
        "  return cv2.medianBlur(img, size)\n",
        "\n",
        "# Get list of labeled images\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name))\n",
        "with open(\"Samples.txt\", 'r') as f:\n",
        "  date_uuids = f.read().splitlines()\n",
        "\n",
        "# Iterate through the list\n",
        "for date_uuid in date_uuids[:20]:\n",
        "  # Load the image and labels\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "  image = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3])), cv2.COLOR_BGR2RGB)\n",
        "  image[:, :, 2] = np.zeros((image_dimension, image_dimension)) # Remove the blue channel\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "  concentration = cv2.imread(concentration_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]), cv2.IMREAD_GRAYSCALE)\n",
        "  uncertainty = cv2.imread(uncertainty_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "  # Plot the results\n",
        "  plt.figure(figsize=(30,5))\n",
        "  plt.suptitle(date_uuid)\n",
        "\n",
        "  plt.subplot(1,5,1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(\"Resampled SAR Image\")\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1,5,2)\n",
        "  plt.imshow(concentration, cmap='hot', vmin=0, vmax=100)\n",
        "  plt.title(\"Concentration Patch Label\")\n",
        "  plt.axis('off')\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.subplot(1,5,3)\n",
        "  plt.imshow(median_filter(concentration), cmap='hot', vmin=0, vmax=100)\n",
        "  plt.title(\"Concentration Patch Label (Filtered)\")\n",
        "  plt.axis('off')\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.subplot(1,5,4)\n",
        "  plt.imshow(uncertainty, cmap='hot', vmin=0, vmax=100)\n",
        "  plt.title(\"Uncertainty Patch Label\")\n",
        "  plt.axis('off')\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.subplot(1,5,5)\n",
        "  plt.imshow(median_filter(uncertainty), cmap='hot', vmin=0, vmax=100)\n",
        "  plt.title(\"Uncertainty Patch Label (Filtered)\")\n",
        "  plt.axis('off')\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC8i7uM_ZXR9",
        "colab_type": "text"
      },
      "source": [
        "## Data Curation Investigation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA1c_1HQZcGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def median_filter(img, size=5):\n",
        "  return cv2.medianBlur(img, size)\n",
        "\n",
        "# Get list of labeled images\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "with open(\"Successful_Labels.txt\", 'r') as f:\n",
        "  date_uuids = f.read().splitlines()\n",
        "\n",
        "# Iterate through the list\n",
        "for date_uuid in date_uuids[:50]:\n",
        "  # Load the image and labels\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "  original = cv2.cvtColor(cv2.imread(sentinel_1_quicklook_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3])), cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3])), cv2.COLOR_BGR2RGB)\n",
        "  image[:, :, 2] = np.zeros((image_dimension, image_dimension)) # Remove the blue channel\n",
        "  os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "  concentration = cv2.imread(concentration_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]), cv2.IMREAD_GRAYSCALE) / 100\n",
        "  uncertainty   = cv2.imread(uncertainty_name.format(date_uuid.split(\"_\")[0], date_uuid.split(\"_\")[1], date_uuid.split(\"_\")[2], date_uuid.split(\"_\")[3]), cv2.IMREAD_GRAYSCALE) / 100\n",
        "\n",
        "  # Print out the variance\n",
        "  print(\"Variance of the concentration label: {:.4f}\\nAspect Ratio (x/y): {:.4f}\".format(np.var(concentration), np.shape(original)[1] / np.shape(original)[0]))\n",
        "\n",
        "  # Plot the results\n",
        "  plt.figure(figsize=(20,4))\n",
        "  plt.suptitle(date_uuid)\n",
        "\n",
        "  plt.subplot(1,4,1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(\"Resampled SAR Image\")\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1,4,2)\n",
        "  plt.imshow(concentration, cmap='hot', vmin=0, vmax=1)\n",
        "  plt.title(\"Concentration Patch Label\")\n",
        "  plt.axis('off')\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.subplot(1,4,3)\n",
        "  plt.imshow(uncertainty, cmap='hot', vmin=0, vmax=1)\n",
        "  plt.title(\"Uncertainty Patch Label\")\n",
        "  plt.axis('off')\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.subplot(1,4,4)\n",
        "  plt.hist(np.reshape(concentration, -1), bins=100, range=(0,1), log=True)\n",
        "  plt.title(\"Concentration Distribution\")\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuytcVdQ05A0",
        "colab_type": "text"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLm8cxpBkdSN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Run to define custom loss functions\n",
        "# Uncertainty weighted versions of MAE & MSE\n",
        "def uncertainty_weighted_MAE(y_true, y_pred):\n",
        "  # y_true contains the concentration label, and the uncertainty as two two channels\n",
        "  y_target    = y_true[:, :, :, :1]\n",
        "  uncertainty = y_true[:, :, :, 1:]\n",
        "  # Calculate the weighted MAE\n",
        "  loss = K.abs(y_pred - y_target)\n",
        "  loss = loss * (K.ones_like(loss) - uncertainty) # Scale the error by the 'certainty' of the label\n",
        "  return K.mean(loss, axis=-1)\n",
        "\n",
        "def uncertainty_weighted_MSE(y_true, y_pred):\n",
        "  # y_true contains the concentration label, and the uncertainty as two two channels\n",
        "  y_target    = y_true[:, :, :, :1]\n",
        "  uncertainty = y_true[:, :, :, 1:]\n",
        "  # Calculate the weighted MSE\n",
        "  loss = K.abs(y_pred - y_target)\n",
        "  loss = loss * (K.ones_like(loss) - uncertainty) # Scale the error by the 'certainty' of the label\n",
        "  return K.mean(K.square(loss), axis=-1)\n",
        "\n",
        "# Uncertainty biased versions of MAE & MSE\n",
        "def uncertainty_biased_MAE(y_true, y_pred):\n",
        "  # y_true contains the concentration label, and the uncertainty as two two channels\n",
        "  y_target    = y_true[:, :, :, :1]\n",
        "  uncertainty = y_true[:, :, :, 1:]\n",
        "  # Calculate the biased MAE\n",
        "  loss = K.abs(y_pred - y_target)\n",
        "  loss = K.maximum(K.zeros_like(loss), (loss - uncertainty))\n",
        "  return K.mean(loss, axis=-1)\n",
        "\n",
        "def uncertainty_biased_MSE(y_true, y_pred):\n",
        "  # y_true contains the concentration label, and the uncertainty as two two channels\n",
        "  y_target    = y_true[:, :, :, :1]\n",
        "  uncertainty = y_true[:, :, :, 1:]\n",
        "  # Calculate the biased MSE\n",
        "  loss = K.abs(y_pred - y_target)\n",
        "  loss = K.maximum(K.zeros_like(loss), (loss - uncertainty))\n",
        "  return K.mean(K.square(loss), axis=-1)\n",
        "\n",
        "# Standard Mean Absolute Error & Mean Squared Error\n",
        "def myMAE(y_true, y_pred):\n",
        "  # y_true contains the concentration label, and the uncertainty as two two channels\n",
        "  y_target = y_true[:, :, :, :1]\n",
        "  # Calculate the MSE\n",
        "  return K.mean(K.abs(y_pred - y_target), axis=-1)\n",
        "\n",
        "def myMSE(y_true, y_pred):\n",
        "  # y_true contains the concentration label, and the uncertainty as two two channels\n",
        "  y_target = y_true[:, :, :, :1]\n",
        "  # Calculate the MSE\n",
        "  return K.mean(K.square(y_pred - y_target), axis=-1)\n",
        "\n",
        "loss_dictionary =  {\"uncertainty_weighted_MAE\":uncertainty_weighted_MAE,\n",
        "                    \"uncertainty_weighted_MSE\":uncertainty_weighted_MSE,\n",
        "                    \"uncertainty_biased_MAE\":uncertainty_biased_MAE,\n",
        "                    \"uncertainty_biased_MSE\":uncertainty_biased_MSE,\n",
        "                    \"myMAE\":myMAE,\n",
        "                    \"myMSE\":myMSE}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbdAuG-gAN2c",
        "colab_type": "text"
      },
      "source": [
        "## Model Definitions - Use these to define new model structures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzrccoPkBG99",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define Simple CNN\n",
        "# Add a convoltional layer to the model\n",
        "def conv_layer(x, filters, dropout_rate):\n",
        "  x = Conv2D(filters, (3,3), padding='same', activation='relu')(x)\n",
        "  if dropout_rate: x = Dropout(dropout_rate)(x)\n",
        "  return x\n",
        "\n",
        "def simple_CNN(optimizer=None, loss=None, metrics=None, input_noise=False, input_shape=None,\n",
        "               layers=None, initial=None, growth_rate=0, dropout_rate=None):\n",
        "  \"\"\"\n",
        "  Creates a simple multi-layer CNN with optional dropout and growth rate.\n",
        "  The function returns a compiled model, which can then be trained\n",
        "  \"\"\"\n",
        "  \n",
        "  # Input layer\n",
        "  img_input = Input(shape=input_shape)\n",
        "  x = img_input\n",
        "\n",
        "  # Optional noise layer applied to the image\n",
        "  if input_noise: x = GaussianNoise(0.10)(x)\n",
        "\n",
        "  # Convolutional layers\n",
        "  for layer_number in range(layers):\n",
        "    x = conv_layer(x, initial + growth_rate * layer_number, dropout_rate)\n",
        "  \n",
        "  # Output layer\n",
        "  x = Conv2D(1, (1,1), padding='same', activation='sigmoid')(x)\n",
        "\n",
        "  # Compile the model, and return it\n",
        "  model = Model(img_input, x)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpwUTUsBDMFP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define U-Net (Modified for Image Mapping)\n",
        "# https://raw.githubusercontent.com/karolzak/keras-unet/master/keras_unet/models/vanilla_unet.py\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.backend import int_shape\n",
        "from keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, Input, concatenate, Cropping2D\n",
        "\n",
        "def vanilla_unet(\n",
        "    optimizer=None,\n",
        "    loss=None,\n",
        "    metrics=None,\n",
        "    input_shape=None,\n",
        "    input_noise=False,\n",
        "    num_classes=1,\n",
        "    dropout=0.5, \n",
        "    filters=64,\n",
        "    num_layers=2,\n",
        "    output_activation='sigmoid'): # 'sigmoid' or 'softmax'\n",
        "\n",
        "    # Build U-Net model\n",
        "    inputs = Input(input_shape)\n",
        "    \n",
        "    if input_noise: x = GaussianNoise(0.10)(inputs)\n",
        "    else: x = inputs   \n",
        "\n",
        "    down_layers = []\n",
        "    for l in range(num_layers):\n",
        "        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n",
        "        down_layers.append(x)\n",
        "        x = MaxPooling2D((2, 2), strides=2) (x)\n",
        "        filters = filters*2 # double the number of filters with each layer\n",
        "\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n",
        "\n",
        "    for conv in reversed(down_layers):\n",
        "        filters //= 2 # decreasing number of filters with each layer \n",
        "        x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same') (x)\n",
        "        \n",
        "        ch, cw = get_crop_shape(int_shape(conv), int_shape(x))\n",
        "        conv = Cropping2D(cropping=(ch, cw))(conv)\n",
        "\n",
        "        x = concatenate([x, conv])\n",
        "        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='same')\n",
        "    \n",
        "    outputs = Conv2D(num_classes, (1, 1), activation=output_activation) (x)    \n",
        "    \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def conv2d_block(\n",
        "    inputs, \n",
        "    use_batch_norm=True, \n",
        "    dropout=0.3, \n",
        "    filters=16, \n",
        "    kernel_size=(3,3), \n",
        "    activation='relu', \n",
        "    kernel_initializer='he_normal', \n",
        "    padding='same'):\n",
        "    \n",
        "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (inputs)\n",
        "    if use_batch_norm:\n",
        "        c = BatchNormalization()(c)\n",
        "    if dropout > 0.0:\n",
        "        c = Dropout(dropout)(c)\n",
        "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (c)\n",
        "    if use_batch_norm:\n",
        "        c = BatchNormalization()(c)\n",
        "    return c\n",
        "\n",
        "def get_crop_shape(target, refer):\n",
        "    # width, the 3rd dimension\n",
        "    cw = target[2] - refer[2]\n",
        "    assert (cw >= 0)\n",
        "    if cw % 2 != 0:\n",
        "        cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
        "    else:\n",
        "        cw1, cw2 = int(cw/2), int(cw/2)\n",
        "    # height, the 2nd dimension\n",
        "    ch = target[1] - refer[1]\n",
        "    assert (ch >= 0)\n",
        "    if ch % 2 != 0:\n",
        "        ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
        "    else:\n",
        "        ch1, ch2 = int(ch/2), int(ch/2)\n",
        "\n",
        "    return (ch1, ch2), (cw1, cw2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLXyLJ_Z-_NE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define DenseNet (Modified for Image Mapping)\n",
        "# Adapted from https://github.com/seasonyc/densenet/blob/master/densenet.py\n",
        "\n",
        "from keras.regularizers import l2\n",
        "\n",
        "def dense_block(x, nb_layers, nb_channels, growth_rate, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Creates a dense block and concatenates inputs\n",
        "    \"\"\"\n",
        "    \n",
        "    x_list = [x]\n",
        "    for i in range(nb_layers):\n",
        "        cb = convolution_block(x, growth_rate, dropout_rate, bottleneck, weight_decay)\n",
        "        x_list.append(cb)\n",
        "        x = Concatenate(axis=-1)(x_list)\n",
        "        nb_channels += growth_rate\n",
        "    return x, nb_channels\n",
        "\n",
        "\n",
        "def convolution_block(x, nb_channels, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Creates a convolution block consisting of BN-ReLU-Conv.\n",
        "    Optional: bottleneck, dropout\n",
        "    \"\"\"\n",
        "    \n",
        "    # Bottleneck\n",
        "    if bottleneck:\n",
        "        bottleneckWidth = 4\n",
        "        x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(nb_channels * bottleneckWidth, (1, 1), use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
        "        # Dropout\n",
        "        if dropout_rate:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "    \n",
        "    # Standard (BN-ReLU-Conv)\n",
        "    x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(nb_channels, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
        "    \n",
        "    # Dropout\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_layer(x, nb_channels, dropout_rate=None, compression=1.0, weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Creates a transition layer between dense blocks as transition, which do convolution.\n",
        "    \"\"\"\n",
        "    \n",
        "    x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(int(nb_channels*compression), (1, 1), padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
        "    \n",
        "    # Adding dropout\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def DenseNet(optimizer=None, loss=None, metrics=None, input_noise=False,\n",
        "             input_shape=None, dense_blocks=3, dense_layers=-1, growth_rate=12, dropout_rate=None,\n",
        "             bottleneck=False, compression=1.0, weight_decay=1e-4, depth=40):\n",
        "    \"\"\"\n",
        "    Creating a DenseNet\n",
        "    \n",
        "    Arguments:\n",
        "        input_shape  : shape of the input images. E.g. (28,28,1) for MNIST    \n",
        "        dense_blocks : amount of dense blocks that will be created (default: 3)    \n",
        "        dense_layers : number of layers in each dense block. You can also use a list for numbers of layers [2,4,3]\n",
        "                       or define only 2 to add 2 layers at all dense blocks. -1 means that dense_layers will be calculated\n",
        "                       by the given depth (default: -1)\n",
        "        growth_rate  : number of filters to add per dense block (default: 12)\n",
        "        dropout_rate : defines the dropout rate that is accomplished after each conv layer (except the first one).\n",
        "                       In the paper the authors recommend a dropout of 0.2 (default: None)\n",
        "        bottleneck   : (True / False) if true it will be added in convolution block (default: False)\n",
        "        compression  : reduce the number of feature-maps at transition layer. In the paper the authors recomment a compression\n",
        "                       of 0.5 (default: 1.0 - will have no compression effect)\n",
        "        weight_decay : weight decay of L2 regularization on weights (default: 1e-4)\n",
        "        depth        : number or layers (default: 40)\n",
        "        \n",
        "    Returns:\n",
        "        Model        : A Keras model instance\n",
        "    \"\"\"\n",
        "\n",
        "    if compression <=0.0 or compression > 1.0:\n",
        "        raise Exception('Compression have to be a value between 0.0 and 1.0. If you set compression to 1.0 it will be turn off.')\n",
        "    \n",
        "    if type(dense_layers) is list:\n",
        "        if len(dense_layers) != dense_blocks:\n",
        "            raise AssertionError('Number of dense blocks have to be same length to specified layers')\n",
        "    elif dense_layers == -1:\n",
        "        if bottleneck:\n",
        "            dense_layers = (depth - (dense_blocks + 1))/dense_blocks // 2\n",
        "        else:\n",
        "            dense_layers = (depth - (dense_blocks + 1))//dense_blocks\n",
        "        dense_layers = [int(dense_layers) for _ in range(dense_blocks)]\n",
        "    else:\n",
        "        dense_layers = [int(dense_layers) for _ in range(dense_blocks)]\n",
        "    \n",
        "    print('Creating DenseNet')\n",
        "    print('#############################################')\n",
        "    print('Dense blocks: %s' % dense_blocks)\n",
        "    print('Layers per dense block: %s' % dense_layers)\n",
        "    print('#############################################')\n",
        "\n",
        "    # Input layer\n",
        "    img_input = Input(shape=input_shape)\n",
        "    x = img_input\n",
        "\n",
        "    # Optional noise layer applied to the image\n",
        "    if input_noise:\n",
        "      x = GaussianNoise(0.10)(x) # standard deviation recommended by paper\n",
        "    \n",
        "    # Initial convolution layer\n",
        "    nb_channels = growth_rate * 2\n",
        "    x = Conv2D(nb_channels, (3,3), padding='same',strides=(1,1), use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
        "    \n",
        "    # Building dense blocks\n",
        "    for block in range(dense_blocks):\n",
        "        \n",
        "        # Add dense block\n",
        "        x, nb_channels = dense_block(x, dense_layers[block], nb_channels, growth_rate, dropout_rate, bottleneck, weight_decay)\n",
        "        \n",
        "        if block < dense_blocks - 1:  # if it's not the last dense block\n",
        "            # Add transition_block\n",
        "            x = transition_layer(x, nb_channels, dropout_rate, compression, weight_decay)\n",
        "            nb_channels = int(nb_channels * compression)\n",
        "    \n",
        "    x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "        \n",
        "    model = Model(img_input, x)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSPaTXK6A0-N",
        "colab_type": "text"
      },
      "source": [
        "## Model Instanciation - Choose a model name, then either import or train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIJeDoTmDnLc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Get available models\n",
        "# Show the available models\n",
        "print(\"The following models are available:\")\n",
        "os.chdir(model_location_path)\n",
        "for m in os.listdir():\n",
        "  if os.path.isdir(m): print(\"\\t\" + m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iw0ThkmpLL-M",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Please choose one and use it in the following cell, or enter a new name to create a new model.\n",
        "model_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Check if the model folder already exists\n",
        "os.chdir(model_location_path)\n",
        "if model_folder in os.listdir():\n",
        "  print(\"{} model folder already exists. Some information may be overwritten.\".format(model_folder))\n",
        "else:\n",
        "  os.mkdir(os.path.join(model_location_path, model_folder))\n",
        "  print(\"This is a new model. {} model folder has been created.\".format(model_folder))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJptAE-M1a3o",
        "colab_type": "text"
      },
      "source": [
        "### Import an existing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmVLr5d1dxF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Get a list of versions available for this model.\n",
        "# Check if the model folder already exists\n",
        "os.chdir(model_location_path)\n",
        "if model_folder in os.listdir():\n",
        "  print(\"{} model folder found.\".format(model_folder))\n",
        "  os.chdir(model_folder)\n",
        "  # Show saved models first\n",
        "  if \"Saved Models\" in os.listdir():\n",
        "    print(\"\\nHere are the models you've saved previously:\")\n",
        "    for f in sorted(os.listdir(\"Saved Models\")):\n",
        "      if f[-4:] == \"hdf5\": print(\"\\t\" + f)\n",
        "    print(\"You can choose any of these to import, or see the most recent saved checkpoints below.\")\n",
        "\n",
        "  # Show intermediate models first\n",
        "  if \"Intermediate Epochs\" in os.listdir():\n",
        "    print(\"\\nHere are the intermediate epoch models you've saved previously:\")\n",
        "    for f in sorted(os.listdir(\"Intermediate Epochs\")):\n",
        "      if f[-4:] == \"hdf5\": print(\"\\t\" + f)\n",
        "    print(\"You can choose any of these to import, or see the most recent saved checkpoints below.\")\n",
        "  \n",
        "  # Show all files in the checkpoint directory\n",
        "  if \"Checkpoints\" in os.listdir():\n",
        "    if len(os.listdir(\"Checkpoints\")) > 0:\n",
        "      print(\"\\nHere are the available checkpoints:\")\n",
        "      for f in os.listdir(\"Checkpoints\"):\n",
        "        print(\"\\t\" + f)\n",
        "      print(\"\\nPlease choose which file you'd like to import.\")\n",
        "    else:\n",
        "      print(\"\\nNo model checkpoints have been saved.\")\n",
        "else:\n",
        "  print(\"Cannot find {} model folder. Please check that the model has been created.\".format(model_folder))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p68HEaCI3yx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter the model version name here\n",
        "version_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "from keras.models import load_model\n",
        "os.chdir(os.path.join(model_location_path, model_folder))\n",
        "if version_name in os.listdir(\"Checkpoints\"):\n",
        "  print(\"Importing model from recent checkpoints.\")\n",
        "  os.chdir(\"Checkpoints\")\n",
        "  model = load_model(version_name,\n",
        "                     custom_objects=loss_dictionary)\n",
        "  print(\"Model imported\")\n",
        "elif version_name in os.listdir(\"Saved Models\"):\n",
        "  print(\"Importing model from saved models.\")\n",
        "  os.chdir(\"Saved Models\")\n",
        "  model = load_model(version_name,\n",
        "                     custom_objects=loss_dictionary)\n",
        "  print(\"Model imported\")\n",
        "elif version_name in os.listdir(\"Intermediate Epochs\"):\n",
        "  print(\"Importing model from intermediate epoch models.\")\n",
        "  os.chdir(\"Intermediate Epochs\")\n",
        "  model = load_model(version_name,\n",
        "                     custom_objects=loss_dictionary)\n",
        "  print(\"Model imported\")\n",
        "else:\n",
        "  print(\"File not found. Please check the version_name and try again.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvVQHjdwLEix",
        "colab_type": "text"
      },
      "source": [
        "### Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8yJmrwokO1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, TerminateOnNaN, Callback\n",
        "import keras.backend as K\n",
        "\n",
        "# Check the model folder file strucure\n",
        "os.chdir(os.path.join(model_location_path, model_folder))\n",
        "if \"Saved Models\" not in os.listdir(): os.mkdir(\"Saved Models\")\n",
        "if \"Checkpoints\" not in os.listdir(): os.mkdir(\"Checkpoints\")\n",
        "if len(os.listdir(\"Checkpoints\")) == 0:\n",
        "  # OK to train\n",
        "\n",
        "  # Create callbacks\n",
        "  model_description = \"Model Name\"\n",
        "\n",
        "  model_checkpoint = ModelCheckpoint(os.path.join(\"Checkpoints\", model_description + \" - {epoch:03d}.hdf5\"),\n",
        "                                    monitor='loss',\n",
        "                                    verbose=1,\n",
        "                                    save_best_only=True)\n",
        "\n",
        "  terminate_on_NaN = TerminateOnNaN()\n",
        "\n",
        "  # Define the model\n",
        "\n",
        "  # SimpleCNN\n",
        "  # model = simple_CNN(optimizer=Adam(lr=1e-4),\n",
        "  #                   loss=uncertainty_weighted_MAE,\n",
        "  #                   metrics=[uncertainty_weighted_MSE, uncertainty_biased_MAE, uncertainty_biased_MSE, myMAE, myMSE],\n",
        "  #                   input_noise=False,\n",
        "  #                   input_shape=(image_dimension, image_dimension, 2),\n",
        "  #                   layers=10,\n",
        "  #                   initial=32,\n",
        "  #                   growth_rate=32,\n",
        "  #                   dropout_rate=0.2)\n",
        "\n",
        "  # UNet\n",
        "  # model = vanilla_unet(optimizer=Adam(lr=1e-4),\n",
        "  #                      loss=uncertainty_weighted_MAE,\n",
        "  #                      metrics=[uncertainty_weighted_MSE, uncertainty_biased_MAE, uncertainty_biased_MSE, myMAE, myMSE],\n",
        "  #                      input_shape=(image_dimension, image_dimension, 2),\n",
        "  #                      input_noise=False,\n",
        "  #                      num_layers=4,\n",
        "  #                      filters=128,\n",
        "  #                      dropout=0.5)\n",
        "\n",
        "  # DenseNet\n",
        "  # model = DenseNet(input_shape=(image_dimension, image_dimension, 2),\n",
        "  #                  optimizer=Adam(lr=1e-4), # been using \"adam\" or Adam(lr=1e-4)\n",
        "  #                  loss=uncertainty_weighted_MAE,\n",
        "  #                  metrics=[uncertainty_weighted_MSE, uncertainty_biased_MAE, uncertainty_biased_MSE, myMAE, myMSE],\n",
        "  #                  input_noise=False,\n",
        "  #                  dense_blocks=4,\n",
        "  #                  dense_layers=8,\n",
        "  #                  growth_rate=8,\n",
        "  #                  dropout_rate=0.2,\n",
        "  #                  bottleneck=False,\n",
        "  #                  compression=0.5,\n",
        "  #                  weight_decay=0)\n",
        "\n",
        "\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit_generator(train_sequence,\n",
        "                                epochs=50,\n",
        "                                initial_epoch=32,\n",
        "                                verbose=1, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "                                callbacks=[model_checkpoint, terminate_on_NaN],\n",
        "                                validation_data=test_sequence,\n",
        "                                max_queue_size=10, # allowable generator queue\n",
        "                                workers=10, # number of processes allowed (used by generator)\n",
        "                                use_multiprocessing=True, # allow multiple threads for generator\n",
        "                                shuffle=False)\n",
        "\n",
        "  # The history object can be used to plot the progress of the training.\n",
        "  plt.figure(figsize=(10,10))\n",
        "\n",
        "  plt.subplot(3,2,1)\n",
        "  plt.plot(history.history['loss'],  color='r')\n",
        "  plt.plot(history.history['val_loss'],  color='b')\n",
        "  plt.title(\"Uncertainty Weighted MAE\")\n",
        "  plt.xlabel(\"Epoch Number\")\n",
        "  plt.ylabel(\"Weighted MAE\")\n",
        "  plt.legend(['Train', 'Test'], loc='upper right')\n",
        "  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "  plt.subplot(3,2,2)\n",
        "  plt.plot(history.history['uncertainty_weighted_MSE'],  color='r')\n",
        "  plt.plot(history.history['val_uncertainty_weighted_MSE'],  color='b')\n",
        "  plt.title(\"Uncertainty Weighted MSE\")\n",
        "  plt.xlabel(\"Epoch Number\")\n",
        "  plt.ylabel(\"Weighted MSE\")\n",
        "  plt.legend(['Train', 'Test'], loc='upper right')\n",
        "  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "  plt.subplot(3,2,3)\n",
        "  plt.plot(history.history['uncertainty_biased_MAE'],  color='r')\n",
        "  plt.plot(history.history['val_uncertainty_biased_MAE'],  color='b')\n",
        "  plt.title(\"Uncertainty Biased MAE\")\n",
        "  plt.xlabel(\"Epoch Number\")\n",
        "  plt.ylabel(\"Biased MAE\")\n",
        "  plt.legend(['Train', 'Test'], loc='upper right')\n",
        "  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "  plt.subplot(3,2,4)\n",
        "  plt.plot(history.history['uncertainty_biased_MSE'],  color='r')\n",
        "  plt.plot(history.history['val_uncertainty_biased_MSE'],  color='b')\n",
        "  plt.title(\"Uncertainty Biased MSE\")\n",
        "  plt.xlabel(\"Epoch Number\")\n",
        "  plt.ylabel(\"Biased MSE\")\n",
        "  plt.legend(['Train', 'Test'], loc='upper right')\n",
        "  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "  plt.subplot(3,2,5)\n",
        "  plt.plot(history.history['myMAE'],  color='r')\n",
        "  plt.plot(history.history['val_myMAE'],  color='b')\n",
        "  plt.title(\"Mean Absolute Error\")\n",
        "  plt.xlabel(\"Epoch Number\")\n",
        "  plt.ylabel(\"MAE\")\n",
        "  plt.legend(['Train', 'Test'], loc='upper right')\n",
        "  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "  plt.subplot(3,2,6)\n",
        "  plt.plot(history.history['myMSE'],  color='r')\n",
        "  plt.plot(history.history['val_myMSE'],  color='b')\n",
        "  plt.title(\"Mean Squared Error\")\n",
        "  plt.xlabel(\"Epoch Number\")\n",
        "  plt.ylabel(\"MSE\")\n",
        "  plt.legend(['Train', 'Test'], loc='upper right')\n",
        "  plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.savefig(\"Saved Models/{}.png\".format(model_description))\n",
        "\n",
        "  # Save the history dictionary to file for later analysis\n",
        "  history_dataframe = pd.DataFrame(history.history)\n",
        "  history_dataframe.to_csv(\"Saved Models/{}.txt\".format(model_description), header=True, index=False, mode='w')\n",
        "\n",
        "else:\n",
        "  # Not OK to train\n",
        "  print(\"Please clear the Checkpoints folder before training a new model.\")\n",
        "  print(os.listdir(\"Checkpoints\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNZT4N01tXy",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NWd9XjQ_YTH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ##Evaluate Model Using Test Data\n",
        "test_on_ignored_samples = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Decide which data to test on\n",
        "if test_on_ignored_samples:\n",
        "  seq = test_ignored_sequence\n",
        "else:\n",
        "  seq = test_sequence\n",
        "\n",
        "# Evaluate and print results\n",
        "evaluation = model.evaluate_generator(test_sequence, verbose=1)\n",
        "print(model.metrics_names)\n",
        "print(evaluation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qeljC-3KeHI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Generate a Set of Predictions\n",
        "batches_to_display = 1 #@param {type:\"integer\"}\n",
        "display_all_batches = False #@param {type:\"boolean\"}\n",
        "\n",
        "os.chdir(os.path.join(model_location_path, model_folder))\n",
        "if \"Predictions\" in os.listdir():\n",
        "  os.chdir(\"Predictions\")\n",
        "  for f in os.listdir():\n",
        "    os.remove(f)\n",
        "else:\n",
        "  os.mkdir(\"Predictions\")\n",
        "\n",
        "predictions = model.predict_generator(predict_sequence, verbose=1)\n",
        "\n",
        "for batch_number in range(len(predict_sequence) if display_all_batches else batches_to_display):\n",
        "  batch_images      = predict_sequence.getImageBatch(batch_number)\n",
        "  batch_labels      = predict_sequence.getLabelBatch(batch_number)\n",
        "  batch_uncertainty = predict_sequence.getUncertaintyBatch(batch_number)\n",
        "  batch_uuids       = predict_sequence.getDateUUID(batch_number)\n",
        "\n",
        "  for img_number in range(len(batch_images)):\n",
        "    plt.figure(figsize=(20,4))\n",
        "    plt.suptitle(batch_uuids[img_number], fontsize=18)\n",
        "\n",
        "    plt.subplot(1,4,1)\n",
        "    plt.imshow(batch_images[img_number], vmin=0, vmax=255)\n",
        "    plt.title(\"Input\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,4,2)\n",
        "    plt.imshow(predictions[batch_number * batch_size + img_number][:, :, 0], cmap=\"hot\", vmin=0, vmax=1)\n",
        "    plt.title(\"Model Prediction\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.subplot(1,4,3)\n",
        "    plt.imshow(batch_labels[img_number][:, :, 0], cmap=\"hot\", vmin=0, vmax=1)\n",
        "    plt.title(\"Target\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.subplot(1,4,4)\n",
        "    plt.imshow(batch_uncertainty[img_number][:, :, 0], cmap=\"hot\", vmin=0, vmax=1)\n",
        "    plt.title(\"Uncertainty\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.colorbar()\n",
        "    \n",
        "    os.chdir(os.path.join(model_location_path, model_folder, \"Predictions\"))\n",
        "    plt.savefig(batch_uuids[img_number])\n",
        "\n",
        "    # clear_output()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f86T4x4fWjSY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ##Generate an Individual Prediction\n",
        "date_uuid = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Get the image path\n",
        "year  = date_uuid.split(\"_\")[0]\n",
        "month = date_uuid.split(\"_\")[1]\n",
        "day   = date_uuid.split(\"_\")[2]\n",
        "uuid  = date_uuid.split(\"_\")[3]\n",
        "\n",
        "# Load the resampled SAR image\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, sentinel_1_folder))\n",
        "image = np.zeros((1, image_dimension, image_dimension, 2))\n",
        "image[0, :, :, :2] = cv2.cvtColor(cv2.imread(sentinel_1_resampled_name.format(year, month, day, uuid)), cv2.COLOR_BGR2RGB)[:, :, :2] / 255\n",
        "disp_image = np.zeros((image_dimension, image_dimension, 3))\n",
        "disp_image[:, :, :2] = image[0, :, :, :]\n",
        "\n",
        "# Load the concentration label, and uncertainty patch\n",
        "os.chdir(os.path.join(dataset_location_path, dataset_name, cmems_label_folder))\n",
        "concentration = cv2.medianBlur(cv2.imread(concentration_name.format(year, month, day, uuid), cv2.IMREAD_GRAYSCALE), 5) / 100\n",
        "uncertainty   = cv2.medianBlur(cv2.imread(uncertainty_name.format(year, month, day, uuid),   cv2.IMREAD_GRAYSCALE), 5) / 100\n",
        "\n",
        "# Get a prediction from the model\n",
        "prediction = model.predict(image)[0, :, :, 0]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(20,4))\n",
        "plt.suptitle(date_uuid, fontsize=18)\n",
        "\n",
        "plt.subplot(1,4,1)\n",
        "plt.imshow(disp_image, vmin=0, vmax=1)\n",
        "plt.title(\"SAR Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,4,2)\n",
        "plt.imshow(prediction, cmap=\"hot\", vmin=0, vmax=1)\n",
        "plt.title(\"Model Prediction\")\n",
        "plt.axis('off')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,4,3)\n",
        "plt.imshow(concentration, cmap=\"hot\", vmin=0, vmax=1)\n",
        "plt.title(\"Target\")\n",
        "plt.axis('off')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,4,4)\n",
        "plt.imshow(uncertainty, cmap=\"hot\", vmin=0, vmax=1)\n",
        "plt.title(\"Uncertainty\")\n",
        "plt.axis('off')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lXJQA7ALdaE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ## Generate Training Graph\n",
        "model_type = \"CNN\" #@param [\"CNN\", \"UNet\", \"DenseNet\"]\n",
        "model_variant = \"N_N\" #@param [\"N_N\", \"S_S\", \"NS_S\"]\n",
        "\n",
        "os.chdir(os.path.join(model_location_path, model_type, \"Saved Models\"))\n",
        "if model_variant is not \"NS_S\":\n",
        "  # Handle the normal history files\n",
        "  for f in os.listdir():\n",
        "    if \"{}_{}.txt\".format(model_type, model_variant) in f: h = pd.read_csv(f)\n",
        "    if \"{}_{}_A.txt\".format(model_type, model_variant) in f: h_A = pd.read_csv(f)\n",
        "else:\n",
        "  # Need to get intermediate history file, and final history file\n",
        "\n",
        "  # Load the base model history\n",
        "  for f in os.listdir():\n",
        "    if \"{}_N_N.txt\".format(model_type) in f: h = pd.read_csv(f)\n",
        "    if \"{}_N_N_A.txt\".format(model_type) in f: h_A = pd.read_csv(f)\n",
        "\n",
        "  # Only keep the first 32 epochs\n",
        "  h   = h[:32]\n",
        "  h_A = h_A[:32]\n",
        "\n",
        "  # Append the results from the second part of training\n",
        "  for f in os.listdir():\n",
        "    if \"{}_{}.txt\".format(model_type, model_variant) in f: h = h.append(pd.read_csv(f), ignore_index=True)\n",
        "    if \"{}_{}_A.txt\".format(model_type, model_variant) in f: h_A = h_A.append(pd.read_csv(f), ignore_index=True)\n",
        "\n",
        "# Plot the training graphs\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.suptitle(\"{}_{}\".format(model_type, model_variant), fontsize=18)\n",
        "\n",
        "# Weighted MAE\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(h['loss'],       color='r', linestyle='-')\n",
        "plt.plot(h_A['loss'],     color='r', linestyle='--')\n",
        "plt.plot(h['val_loss'],   color='b', linestyle='-')\n",
        "plt.plot(h_A['val_loss'], color='b', linestyle='--')\n",
        "plt.title(\"Uncertainty Weighted MAE\")\n",
        "plt.xlabel(\"Epoch Number\")\n",
        "plt.ylabel(\"Weighted MAE\")\n",
        "plt.legend(['Train', 'Train (Augmented)', 'Test', 'Test (Augmented)'], loc='upper right')\n",
        "plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "# MAE\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(h['myMAE'],      color='r', linestyle='-')\n",
        "plt.plot(h_A['myMAE'],      color='r', linestyle='--')\n",
        "plt.plot(h['val_myMAE'],  color='b', linestyle='-')\n",
        "plt.plot(h_A['val_myMAE'],  color='b', linestyle='--')\n",
        "plt.title(\"Mean Absolute Error\")\n",
        "plt.xlabel(\"Epoch Number\")\n",
        "plt.ylabel(\"MAE\")\n",
        "plt.legend(['Train', 'Train (Augmented)', 'Test', 'Test (Augmented)'], loc='upper right')\n",
        "plt.gca().set_facecolor((0.9, 0.9, 0.9))\n",
        "\n",
        "# Show plots\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zct_o6LhV5pZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}